{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\ud83e\udd8b LSDIPro SS2025 \ud83d\udcc4 SilkMoth: An Efficient Method for Finding Related Sets A project inspired by the SilkMoth paper, exploring efficient techniques for related set discovery. \ud83d\udc65 Team Members Andreas Wilms Sarra Daknou Amina Iqbal Jakob Berschneider \ud83d\udcd8 Project Documentation \ud83d\udd01 Core Pipeline Steps Figure 1. SILKMOTH pipeline framework. Source: Deng et al., \"SILKMOTH: An Efficient Method for Finding Related Sets with Maximum Matching Constraints\", VLDB 2017. Licensed under CC BY-NC-ND 4.0. 1. Tokenization Each element in every set is tokenized based on the selected similarity function: Jaccard Similarity : Elements are split into whitespace-delimited tokens. Edit Similarity : Elements are split into overlapping q -grams (e.g., 3-grams). 2. Inverted Index Construction An inverted index is built from the reference set R to map each token to a list of (set, element) pairs in which it occurs. This allows fast lookup of candidate sets that share tokens with a query. 3. Signature Generation A signature is a subset of tokens selected from each set such that: Any related set must share at least one signature token. Signature size is minimized to reduce candidate space. Signature selection heuristics (e.g., cost/value greedy ranking) are used to approximate the optimal valid signature, which is NP-complete to compute exactly. 4. Candidate Selection For each set R , we retrieve from the inverted index all sets S that share at least one token with R \u2019s signature. These become the candidate sets for further evaluation. 5. Refinement Filters Two filters reduce false positives among the candidates: Check Filter : Uses an upper bound on similarity to eliminate sets that cannot meet the threshold. Nearest Neighbor Filter : Approximates the maximum matching score using the nearest neighbor similarity for each element in R . 6. Verification via Maximum Matching For the remaining candidates, we compute the maximum weighted bipartite matching between elements of R and S , using the chosen similarity function as edge weights. Only sets whose matching score meets or exceeds a threshold \u03b4 are considered related . \ud83e\uddea Modes of Operation Discovery Mode : Compare all pairs of sets to find all related set pairs. Use Case : When you want to check which sets (e.g., columns in a database) are related to a specific reference set. Search Mode : Given a reference set, find all sets related to it. Use Case : When you want to find all related set pairs in a dataset, for tasks like schema matching or entity deduplication. \ud83d\udcd0 Supported Similarity Functions Jaccard Similarity Edit Similarity (Levenshtein-based) Optional minimum similarity threshold \u03b1 can be enforced on element comparisons. Installing from source Run pip install src/ to install (optional) Run python -m unittest discover -s src/silkmoth/test -p \"*.py\" to execute the unit tests Running the experiment Run python experiments/run.py to execute the experiment","title":"Home"},{"location":"#lsdipro-ss2025","text":"","title":"\ud83e\udd8b LSDIPro SS2025"},{"location":"#silkmoth-an-efficient-method-for-finding-related-sets","text":"A project inspired by the SilkMoth paper, exploring efficient techniques for related set discovery.","title":"\ud83d\udcc4 SilkMoth: An Efficient Method for Finding Related Sets"},{"location":"#team-members","text":"Andreas Wilms Sarra Daknou Amina Iqbal Jakob Berschneider","title":"\ud83d\udc65 Team Members"},{"location":"#project-documentation","text":"","title":"\ud83d\udcd8 Project Documentation"},{"location":"#core-pipeline-steps","text":"Figure 1. SILKMOTH pipeline framework. Source: Deng et al., \"SILKMOTH: An Efficient Method for Finding Related Sets with Maximum Matching Constraints\", VLDB 2017. Licensed under CC BY-NC-ND 4.0.","title":"\ud83d\udd01 Core Pipeline Steps"},{"location":"#1-tokenization","text":"Each element in every set is tokenized based on the selected similarity function: Jaccard Similarity : Elements are split into whitespace-delimited tokens. Edit Similarity : Elements are split into overlapping q -grams (e.g., 3-grams).","title":"1. Tokenization"},{"location":"#2-inverted-index-construction","text":"An inverted index is built from the reference set R to map each token to a list of (set, element) pairs in which it occurs. This allows fast lookup of candidate sets that share tokens with a query.","title":"2. Inverted Index Construction"},{"location":"#3-signature-generation","text":"A signature is a subset of tokens selected from each set such that: Any related set must share at least one signature token. Signature size is minimized to reduce candidate space. Signature selection heuristics (e.g., cost/value greedy ranking) are used to approximate the optimal valid signature, which is NP-complete to compute exactly.","title":"3. Signature Generation"},{"location":"#4-candidate-selection","text":"For each set R , we retrieve from the inverted index all sets S that share at least one token with R \u2019s signature. These become the candidate sets for further evaluation.","title":"4. Candidate Selection"},{"location":"#5-refinement-filters","text":"Two filters reduce false positives among the candidates: Check Filter : Uses an upper bound on similarity to eliminate sets that cannot meet the threshold. Nearest Neighbor Filter : Approximates the maximum matching score using the nearest neighbor similarity for each element in R .","title":"5. Refinement Filters"},{"location":"#6-verification-via-maximum-matching","text":"For the remaining candidates, we compute the maximum weighted bipartite matching between elements of R and S , using the chosen similarity function as edge weights. Only sets whose matching score meets or exceeds a threshold \u03b4 are considered related .","title":"6. Verification via Maximum Matching"},{"location":"#modes-of-operation","text":"Discovery Mode : Compare all pairs of sets to find all related set pairs. Use Case : When you want to check which sets (e.g., columns in a database) are related to a specific reference set. Search Mode : Given a reference set, find all sets related to it. Use Case : When you want to find all related set pairs in a dataset, for tasks like schema matching or entity deduplication.","title":"\ud83e\uddea Modes of Operation"},{"location":"#supported-similarity-functions","text":"Jaccard Similarity Edit Similarity (Levenshtein-based) Optional minimum similarity threshold \u03b1 can be enforced on element comparisons.","title":"\ud83d\udcd0 Supported Similarity Functions"},{"location":"#installing-from-source","text":"Run pip install src/ to install (optional) Run python -m unittest discover -s src/silkmoth/test -p \"*.py\" to execute the unit tests","title":"Installing from source"},{"location":"#running-the-experiment","text":"Run python experiments/run.py to execute the experiment","title":"Running the experiment"},{"location":"pages/candidate_selector/","text":"CandidateSelector Source code in silkmoth/candidate_selector.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 class CandidateSelector : def __init__ ( self , similarity_func , sim_metric , related_thresh , sim_thresh = 0.0 ): \"\"\" Args: similarity_func (callable): Similarity function phi(r, s) (e.g., Jaccard). sim_metric (callable): Similarity metric related(R, S) (e.g., contain). related_thresh (float): Relatedness threshold delta. sim_thresh (float): Similarity threshold alpha. \"\"\" self . similarity = similarity_func self . sim_metric = sim_metric self . delta = related_thresh self . alpha = sim_thresh def get_candidates ( self , signature , inverted_index , ref_size ) -> set : \"\"\" Retrieve candidate set indices using token signature lookup. Args: signature (list): Signature tokens for a reference set. inverted_index (InvertedIndex): Instance of the custom InvertedIndex class. ref_size (int): Size of set R. Returns: set: Indices of candidate sets containing at least one signature token. \"\"\" candidates = set () for token in signature : try : idx_list = inverted_index . get_indexes ( token ) for set_idx , _ in idx_list : src_size = len ( inverted_index . get_set ( set_idx )) if self . verify_size ( ref_size , src_size ): candidates . add ( set_idx ) except ValueError : # token not found in inverted index; safely ignore continue return candidates def verify_size ( self , ref_size , src_size ) -> bool : \"\"\" Checks if sets can be related based on their sizes. Set-Containment is only defined for |R|<=|S|. For Set-Similarity we should compare only similar size sets. Args: ref_size (int): Size of set R. src_size (int): Size of (possible) set S. Returns: bool: True if both sets could be related based on their size, False otherwise. \"\"\" # case 1: Set-Containment if self . sim_metric == contain and ref_size > src_size : return False # case 2: Set-Similarity if self . sim_metric == similar : if min ( ref_size , src_size ) < self . delta * max ( ref_size , src_size ): return False return True def check_filter ( self , R , K , candidates , inverted_index ) -> tuple : \"\"\" Apply check filter to prune weak candidate sets. Args: R (list of list): Tokenized reference set. K (set): Flattened signature tokens. candidates (set): Candidate set indices from get_candidates(). inverted_index (InvertedIndex): For retrieving sets. Returns: tuple: set: Candidate indices that pass the check filter. dict: c_idx -> dict{r_idx -> max_sim}. \"\"\" filtered = set () match_map = dict () k_i_sets = [ set ( r_i ) . intersection ( K ) for r_i in R ] for c_idx in candidates : matched = self . create_match_map ( R , k_i_sets , c_idx , inverted_index ) if matched : filtered . add ( c_idx ) match_map [ c_idx ] = matched return filtered , match_map def create_match_map ( self , R , k_i_sets , c_idx , inverted_index ) -> dict : \"\"\" Create a match map for a specific candidate index. Args: R (list of list): Tokenized reference set. k_i_sets (list of sets): Unflattened signature. c_idx (int): Candidate set index. inverted_index (InvertedIndex): For retrieving sets. Returns: dict: r_idx -> max_sim for matched reference sets. \"\"\" S = inverted_index . get_set ( c_idx ) matched = {} for r_idx , ( r_i , k_i ) in enumerate ( zip ( R , k_i_sets )): if not r_i or not k_i : continue threshold = ( len ( r_i ) - len ( k_i )) / len ( r_i ) r_set = set ( r_i ) max_sim = 0.0 for token in k_i : try : entries = inverted_index . get_indexes_binary ( token , c_idx ) for s_idx , e_idx in entries : if s_idx != c_idx : continue s = S [ e_idx ] sim = self . similarity ( r_set , set ( s ), self . alpha ) if sim >= threshold : max_sim = max ( max_sim , sim ) except ValueError : continue if max_sim >= threshold : matched [ r_idx ] = max_sim return matched def _nn_search ( self , r_set , S , c_idx , inverted_index ): \"\"\" Find the maximum similarity between r and elements s \u2208 S[C] that share at least one token with r using the inverted index for efficiency. Args: r_set (set): Reference element tokens. S (list of list): Elements of candidate set S[c_idx]. c_idx (int): Index of candidate set in inverted index. inverted_index (InvertedIndex): For fetching token locations. Returns: float: Maximum similarity between r and any s \u2208 S[c_idx]. \"\"\" # seen = set() max_sim = 0.0 for token in r_set : try : entries = inverted_index . get_indexes_binary ( token , c_idx ) for s_idx , e_idx in entries : if s_idx != c_idx : continue s = S [ e_idx ] sim = self . similarity ( r_set , set ( s ), self . alpha ) max_sim = max ( max_sim , sim ) except ValueError : continue return max_sim def nn_filter ( self , R , K , candidates , inverted_index , threshold , match_map ) -> set : \"\"\" Nearest Neighbor Filter (Algorithm 2 from SilkMoth paper). Args: R (list of list): Tokenized reference set. K (set): Flattened signature tokens. candidates (set): Candidate indices from check filter. inverted_index (InvertedIndex): To retrieve sets and indexes. threshold (float): Relatedness threshold \u03b4 (between 0 and 1). match_map (dict): Maps candidate set index to matched r\u1d62 indices and their max sim (from check filter). Returns: set: Final filtered candidate indices that pass the NN filter. \"\"\" n = len ( R ) theta = threshold * n k_i_sets = [ set ( r_i ) . intersection ( K ) for r_i in R ] final_filtered = set () total_init = 0 for r_idx , r_i in enumerate ( R ): if not r_i : continue base_loss = ( len ( r_i ) - len ( k_i_sets [ r_idx ])) / len ( r_i ) total_init += base_loss for c_idx in candidates : S = inverted_index . get_set ( c_idx ) if self . alpha > 0 : S_tokens = set () for s in S : S_tokens . update ( s ) # Check if match_map is provided, otherwise create it if match_map is None : matched = self . create_match_map ( R , K , c_idx , inverted_index ) else : matched = match_map . get ( c_idx , {}) # Step 1: initialize total estimate total = total_init # Step 2: for matched r\u1d62, computational reuse of sim and adjust total for r_idx , sim in matched . items (): r_i = R [ r_idx ] if not r_i : continue k_i = k_i_sets [ r_idx ] base_loss = ( len ( r_i ) - len ( k_i )) / len ( r_i ) total += sim - base_loss # Step 3: for non-matched r\u1d62, compute NN and adjust total for r_idx in set ( range ( n )) - matched . keys (): r_i = R [ r_idx ] if not r_i : continue k_i = k_i_sets [ r_idx ] base_loss = ( len ( r_i ) - len ( k_i )) / len ( r_i ) r_set = set ( r_i ) # Case alpha > 0 if ( self . alpha > 0 and len ( k_i ) >= floor (( 1 - self . alpha ) * len ( r_i )) + 1 and k_i . isdisjoint ( S_tokens )): nn_sim = 0 else : nn_sim = self . _nn_search ( r_set , S , c_idx , inverted_index ) total += nn_sim - base_loss if total < theta : break if total >= theta : final_filtered . add ( c_idx ) return final_filtered __init__ ( similarity_func , sim_metric , related_thresh , sim_thresh = 0.0 ) Parameters: Name Type Description Default similarity_func callable Similarity function phi(r, s) (e.g., Jaccard). required sim_metric callable Similarity metric related(R, S) (e.g., contain). required related_thresh float Relatedness threshold delta. required sim_thresh float Similarity threshold alpha. 0.0 Source code in silkmoth/candidate_selector.py 6 7 8 9 10 11 12 13 14 15 16 17 def __init__ ( self , similarity_func , sim_metric , related_thresh , sim_thresh = 0.0 ): \"\"\" Args: similarity_func (callable): Similarity function phi(r, s) (e.g., Jaccard). sim_metric (callable): Similarity metric related(R, S) (e.g., contain). related_thresh (float): Relatedness threshold delta. sim_thresh (float): Similarity threshold alpha. \"\"\" self . similarity = similarity_func self . sim_metric = sim_metric self . delta = related_thresh self . alpha = sim_thresh check_filter ( R , K , candidates , inverted_index ) Apply check filter to prune weak candidate sets. Parameters: Name Type Description Default R list of list Tokenized reference set. required K set Flattened signature tokens. required candidates set Candidate set indices from get_candidates(). required inverted_index InvertedIndex For retrieving sets. required Returns: Name Type Description tuple tuple set: Candidate indices that pass the check filter. dict: c_idx -> dict{r_idx -> max_sim}. Source code in silkmoth/candidate_selector.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def check_filter ( self , R , K , candidates , inverted_index ) -> tuple : \"\"\" Apply check filter to prune weak candidate sets. Args: R (list of list): Tokenized reference set. K (set): Flattened signature tokens. candidates (set): Candidate set indices from get_candidates(). inverted_index (InvertedIndex): For retrieving sets. Returns: tuple: set: Candidate indices that pass the check filter. dict: c_idx -> dict{r_idx -> max_sim}. \"\"\" filtered = set () match_map = dict () k_i_sets = [ set ( r_i ) . intersection ( K ) for r_i in R ] for c_idx in candidates : matched = self . create_match_map ( R , k_i_sets , c_idx , inverted_index ) if matched : filtered . add ( c_idx ) match_map [ c_idx ] = matched return filtered , match_map create_match_map ( R , k_i_sets , c_idx , inverted_index ) Create a match map for a specific candidate index. Parameters: Name Type Description Default R list of list Tokenized reference set. required k_i_sets list of sets Unflattened signature. required c_idx int Candidate set index. required inverted_index InvertedIndex For retrieving sets. required Returns: Name Type Description dict dict r_idx -> max_sim for matched reference sets. Source code in silkmoth/candidate_selector.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def create_match_map ( self , R , k_i_sets , c_idx , inverted_index ) -> dict : \"\"\" Create a match map for a specific candidate index. Args: R (list of list): Tokenized reference set. k_i_sets (list of sets): Unflattened signature. c_idx (int): Candidate set index. inverted_index (InvertedIndex): For retrieving sets. Returns: dict: r_idx -> max_sim for matched reference sets. \"\"\" S = inverted_index . get_set ( c_idx ) matched = {} for r_idx , ( r_i , k_i ) in enumerate ( zip ( R , k_i_sets )): if not r_i or not k_i : continue threshold = ( len ( r_i ) - len ( k_i )) / len ( r_i ) r_set = set ( r_i ) max_sim = 0.0 for token in k_i : try : entries = inverted_index . get_indexes_binary ( token , c_idx ) for s_idx , e_idx in entries : if s_idx != c_idx : continue s = S [ e_idx ] sim = self . similarity ( r_set , set ( s ), self . alpha ) if sim >= threshold : max_sim = max ( max_sim , sim ) except ValueError : continue if max_sim >= threshold : matched [ r_idx ] = max_sim return matched get_candidates ( signature , inverted_index , ref_size ) Retrieve candidate set indices using token signature lookup. Parameters: Name Type Description Default signature list Signature tokens for a reference set. required inverted_index InvertedIndex Instance of the custom InvertedIndex class. required ref_size int Size of set R. required Returns: Name Type Description set set Indices of candidate sets containing at least one signature token. Source code in silkmoth/candidate_selector.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def get_candidates ( self , signature , inverted_index , ref_size ) -> set : \"\"\" Retrieve candidate set indices using token signature lookup. Args: signature (list): Signature tokens for a reference set. inverted_index (InvertedIndex): Instance of the custom InvertedIndex class. ref_size (int): Size of set R. Returns: set: Indices of candidate sets containing at least one signature token. \"\"\" candidates = set () for token in signature : try : idx_list = inverted_index . get_indexes ( token ) for set_idx , _ in idx_list : src_size = len ( inverted_index . get_set ( set_idx )) if self . verify_size ( ref_size , src_size ): candidates . add ( set_idx ) except ValueError : # token not found in inverted index; safely ignore continue return candidates nn_filter ( R , K , candidates , inverted_index , threshold , match_map ) Nearest Neighbor Filter (Algorithm 2 from SilkMoth paper). Parameters: Name Type Description Default R list of list Tokenized reference set. required K set Flattened signature tokens. required candidates set Candidate indices from check filter. required inverted_index InvertedIndex To retrieve sets and indexes. required threshold float Relatedness threshold \u03b4 (between 0 and 1). required match_map dict Maps candidate set index to matched r\u1d62 indices and their max sim (from check filter). required Returns: Name Type Description set set Final filtered candidate indices that pass the NN filter. Source code in silkmoth/candidate_selector.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 def nn_filter ( self , R , K , candidates , inverted_index , threshold , match_map ) -> set : \"\"\" Nearest Neighbor Filter (Algorithm 2 from SilkMoth paper). Args: R (list of list): Tokenized reference set. K (set): Flattened signature tokens. candidates (set): Candidate indices from check filter. inverted_index (InvertedIndex): To retrieve sets and indexes. threshold (float): Relatedness threshold \u03b4 (between 0 and 1). match_map (dict): Maps candidate set index to matched r\u1d62 indices and their max sim (from check filter). Returns: set: Final filtered candidate indices that pass the NN filter. \"\"\" n = len ( R ) theta = threshold * n k_i_sets = [ set ( r_i ) . intersection ( K ) for r_i in R ] final_filtered = set () total_init = 0 for r_idx , r_i in enumerate ( R ): if not r_i : continue base_loss = ( len ( r_i ) - len ( k_i_sets [ r_idx ])) / len ( r_i ) total_init += base_loss for c_idx in candidates : S = inverted_index . get_set ( c_idx ) if self . alpha > 0 : S_tokens = set () for s in S : S_tokens . update ( s ) # Check if match_map is provided, otherwise create it if match_map is None : matched = self . create_match_map ( R , K , c_idx , inverted_index ) else : matched = match_map . get ( c_idx , {}) # Step 1: initialize total estimate total = total_init # Step 2: for matched r\u1d62, computational reuse of sim and adjust total for r_idx , sim in matched . items (): r_i = R [ r_idx ] if not r_i : continue k_i = k_i_sets [ r_idx ] base_loss = ( len ( r_i ) - len ( k_i )) / len ( r_i ) total += sim - base_loss # Step 3: for non-matched r\u1d62, compute NN and adjust total for r_idx in set ( range ( n )) - matched . keys (): r_i = R [ r_idx ] if not r_i : continue k_i = k_i_sets [ r_idx ] base_loss = ( len ( r_i ) - len ( k_i )) / len ( r_i ) r_set = set ( r_i ) # Case alpha > 0 if ( self . alpha > 0 and len ( k_i ) >= floor (( 1 - self . alpha ) * len ( r_i )) + 1 and k_i . isdisjoint ( S_tokens )): nn_sim = 0 else : nn_sim = self . _nn_search ( r_set , S , c_idx , inverted_index ) total += nn_sim - base_loss if total < theta : break if total >= theta : final_filtered . add ( c_idx ) return final_filtered verify_size ( ref_size , src_size ) Checks if sets can be related based on their sizes. Set-Containment is only defined for |R|<=|S|. For Set-Similarity we should compare only similar size sets. Parameters: Name Type Description Default ref_size int Size of set R. required src_size int Size of (possible) set S. required Returns: Name Type Description bool bool True if both sets could be related based on their size, False otherwise. Source code in silkmoth/candidate_selector.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def verify_size ( self , ref_size , src_size ) -> bool : \"\"\" Checks if sets can be related based on their sizes. Set-Containment is only defined for |R|<=|S|. For Set-Similarity we should compare only similar size sets. Args: ref_size (int): Size of set R. src_size (int): Size of (possible) set S. Returns: bool: True if both sets could be related based on their size, False otherwise. \"\"\" # case 1: Set-Containment if self . sim_metric == contain and ref_size > src_size : return False # case 2: Set-Similarity if self . sim_metric == similar : if min ( ref_size , src_size ) < self . delta * max ( ref_size , src_size ): return False return True","title":"Candidate Selector"},{"location":"pages/candidate_selector/#silkmoth.candidate_selector.CandidateSelector","text":"Source code in silkmoth/candidate_selector.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 class CandidateSelector : def __init__ ( self , similarity_func , sim_metric , related_thresh , sim_thresh = 0.0 ): \"\"\" Args: similarity_func (callable): Similarity function phi(r, s) (e.g., Jaccard). sim_metric (callable): Similarity metric related(R, S) (e.g., contain). related_thresh (float): Relatedness threshold delta. sim_thresh (float): Similarity threshold alpha. \"\"\" self . similarity = similarity_func self . sim_metric = sim_metric self . delta = related_thresh self . alpha = sim_thresh def get_candidates ( self , signature , inverted_index , ref_size ) -> set : \"\"\" Retrieve candidate set indices using token signature lookup. Args: signature (list): Signature tokens for a reference set. inverted_index (InvertedIndex): Instance of the custom InvertedIndex class. ref_size (int): Size of set R. Returns: set: Indices of candidate sets containing at least one signature token. \"\"\" candidates = set () for token in signature : try : idx_list = inverted_index . get_indexes ( token ) for set_idx , _ in idx_list : src_size = len ( inverted_index . get_set ( set_idx )) if self . verify_size ( ref_size , src_size ): candidates . add ( set_idx ) except ValueError : # token not found in inverted index; safely ignore continue return candidates def verify_size ( self , ref_size , src_size ) -> bool : \"\"\" Checks if sets can be related based on their sizes. Set-Containment is only defined for |R|<=|S|. For Set-Similarity we should compare only similar size sets. Args: ref_size (int): Size of set R. src_size (int): Size of (possible) set S. Returns: bool: True if both sets could be related based on their size, False otherwise. \"\"\" # case 1: Set-Containment if self . sim_metric == contain and ref_size > src_size : return False # case 2: Set-Similarity if self . sim_metric == similar : if min ( ref_size , src_size ) < self . delta * max ( ref_size , src_size ): return False return True def check_filter ( self , R , K , candidates , inverted_index ) -> tuple : \"\"\" Apply check filter to prune weak candidate sets. Args: R (list of list): Tokenized reference set. K (set): Flattened signature tokens. candidates (set): Candidate set indices from get_candidates(). inverted_index (InvertedIndex): For retrieving sets. Returns: tuple: set: Candidate indices that pass the check filter. dict: c_idx -> dict{r_idx -> max_sim}. \"\"\" filtered = set () match_map = dict () k_i_sets = [ set ( r_i ) . intersection ( K ) for r_i in R ] for c_idx in candidates : matched = self . create_match_map ( R , k_i_sets , c_idx , inverted_index ) if matched : filtered . add ( c_idx ) match_map [ c_idx ] = matched return filtered , match_map def create_match_map ( self , R , k_i_sets , c_idx , inverted_index ) -> dict : \"\"\" Create a match map for a specific candidate index. Args: R (list of list): Tokenized reference set. k_i_sets (list of sets): Unflattened signature. c_idx (int): Candidate set index. inverted_index (InvertedIndex): For retrieving sets. Returns: dict: r_idx -> max_sim for matched reference sets. \"\"\" S = inverted_index . get_set ( c_idx ) matched = {} for r_idx , ( r_i , k_i ) in enumerate ( zip ( R , k_i_sets )): if not r_i or not k_i : continue threshold = ( len ( r_i ) - len ( k_i )) / len ( r_i ) r_set = set ( r_i ) max_sim = 0.0 for token in k_i : try : entries = inverted_index . get_indexes_binary ( token , c_idx ) for s_idx , e_idx in entries : if s_idx != c_idx : continue s = S [ e_idx ] sim = self . similarity ( r_set , set ( s ), self . alpha ) if sim >= threshold : max_sim = max ( max_sim , sim ) except ValueError : continue if max_sim >= threshold : matched [ r_idx ] = max_sim return matched def _nn_search ( self , r_set , S , c_idx , inverted_index ): \"\"\" Find the maximum similarity between r and elements s \u2208 S[C] that share at least one token with r using the inverted index for efficiency. Args: r_set (set): Reference element tokens. S (list of list): Elements of candidate set S[c_idx]. c_idx (int): Index of candidate set in inverted index. inverted_index (InvertedIndex): For fetching token locations. Returns: float: Maximum similarity between r and any s \u2208 S[c_idx]. \"\"\" # seen = set() max_sim = 0.0 for token in r_set : try : entries = inverted_index . get_indexes_binary ( token , c_idx ) for s_idx , e_idx in entries : if s_idx != c_idx : continue s = S [ e_idx ] sim = self . similarity ( r_set , set ( s ), self . alpha ) max_sim = max ( max_sim , sim ) except ValueError : continue return max_sim def nn_filter ( self , R , K , candidates , inverted_index , threshold , match_map ) -> set : \"\"\" Nearest Neighbor Filter (Algorithm 2 from SilkMoth paper). Args: R (list of list): Tokenized reference set. K (set): Flattened signature tokens. candidates (set): Candidate indices from check filter. inverted_index (InvertedIndex): To retrieve sets and indexes. threshold (float): Relatedness threshold \u03b4 (between 0 and 1). match_map (dict): Maps candidate set index to matched r\u1d62 indices and their max sim (from check filter). Returns: set: Final filtered candidate indices that pass the NN filter. \"\"\" n = len ( R ) theta = threshold * n k_i_sets = [ set ( r_i ) . intersection ( K ) for r_i in R ] final_filtered = set () total_init = 0 for r_idx , r_i in enumerate ( R ): if not r_i : continue base_loss = ( len ( r_i ) - len ( k_i_sets [ r_idx ])) / len ( r_i ) total_init += base_loss for c_idx in candidates : S = inverted_index . get_set ( c_idx ) if self . alpha > 0 : S_tokens = set () for s in S : S_tokens . update ( s ) # Check if match_map is provided, otherwise create it if match_map is None : matched = self . create_match_map ( R , K , c_idx , inverted_index ) else : matched = match_map . get ( c_idx , {}) # Step 1: initialize total estimate total = total_init # Step 2: for matched r\u1d62, computational reuse of sim and adjust total for r_idx , sim in matched . items (): r_i = R [ r_idx ] if not r_i : continue k_i = k_i_sets [ r_idx ] base_loss = ( len ( r_i ) - len ( k_i )) / len ( r_i ) total += sim - base_loss # Step 3: for non-matched r\u1d62, compute NN and adjust total for r_idx in set ( range ( n )) - matched . keys (): r_i = R [ r_idx ] if not r_i : continue k_i = k_i_sets [ r_idx ] base_loss = ( len ( r_i ) - len ( k_i )) / len ( r_i ) r_set = set ( r_i ) # Case alpha > 0 if ( self . alpha > 0 and len ( k_i ) >= floor (( 1 - self . alpha ) * len ( r_i )) + 1 and k_i . isdisjoint ( S_tokens )): nn_sim = 0 else : nn_sim = self . _nn_search ( r_set , S , c_idx , inverted_index ) total += nn_sim - base_loss if total < theta : break if total >= theta : final_filtered . add ( c_idx ) return final_filtered","title":"CandidateSelector"},{"location":"pages/candidate_selector/#silkmoth.candidate_selector.CandidateSelector.__init__","text":"Parameters: Name Type Description Default similarity_func callable Similarity function phi(r, s) (e.g., Jaccard). required sim_metric callable Similarity metric related(R, S) (e.g., contain). required related_thresh float Relatedness threshold delta. required sim_thresh float Similarity threshold alpha. 0.0 Source code in silkmoth/candidate_selector.py 6 7 8 9 10 11 12 13 14 15 16 17 def __init__ ( self , similarity_func , sim_metric , related_thresh , sim_thresh = 0.0 ): \"\"\" Args: similarity_func (callable): Similarity function phi(r, s) (e.g., Jaccard). sim_metric (callable): Similarity metric related(R, S) (e.g., contain). related_thresh (float): Relatedness threshold delta. sim_thresh (float): Similarity threshold alpha. \"\"\" self . similarity = similarity_func self . sim_metric = sim_metric self . delta = related_thresh self . alpha = sim_thresh","title":"__init__"},{"location":"pages/candidate_selector/#silkmoth.candidate_selector.CandidateSelector.check_filter","text":"Apply check filter to prune weak candidate sets. Parameters: Name Type Description Default R list of list Tokenized reference set. required K set Flattened signature tokens. required candidates set Candidate set indices from get_candidates(). required inverted_index InvertedIndex For retrieving sets. required Returns: Name Type Description tuple tuple set: Candidate indices that pass the check filter. dict: c_idx -> dict{r_idx -> max_sim}. Source code in silkmoth/candidate_selector.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def check_filter ( self , R , K , candidates , inverted_index ) -> tuple : \"\"\" Apply check filter to prune weak candidate sets. Args: R (list of list): Tokenized reference set. K (set): Flattened signature tokens. candidates (set): Candidate set indices from get_candidates(). inverted_index (InvertedIndex): For retrieving sets. Returns: tuple: set: Candidate indices that pass the check filter. dict: c_idx -> dict{r_idx -> max_sim}. \"\"\" filtered = set () match_map = dict () k_i_sets = [ set ( r_i ) . intersection ( K ) for r_i in R ] for c_idx in candidates : matched = self . create_match_map ( R , k_i_sets , c_idx , inverted_index ) if matched : filtered . add ( c_idx ) match_map [ c_idx ] = matched return filtered , match_map","title":"check_filter"},{"location":"pages/candidate_selector/#silkmoth.candidate_selector.CandidateSelector.create_match_map","text":"Create a match map for a specific candidate index. Parameters: Name Type Description Default R list of list Tokenized reference set. required k_i_sets list of sets Unflattened signature. required c_idx int Candidate set index. required inverted_index InvertedIndex For retrieving sets. required Returns: Name Type Description dict dict r_idx -> max_sim for matched reference sets. Source code in silkmoth/candidate_selector.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def create_match_map ( self , R , k_i_sets , c_idx , inverted_index ) -> dict : \"\"\" Create a match map for a specific candidate index. Args: R (list of list): Tokenized reference set. k_i_sets (list of sets): Unflattened signature. c_idx (int): Candidate set index. inverted_index (InvertedIndex): For retrieving sets. Returns: dict: r_idx -> max_sim for matched reference sets. \"\"\" S = inverted_index . get_set ( c_idx ) matched = {} for r_idx , ( r_i , k_i ) in enumerate ( zip ( R , k_i_sets )): if not r_i or not k_i : continue threshold = ( len ( r_i ) - len ( k_i )) / len ( r_i ) r_set = set ( r_i ) max_sim = 0.0 for token in k_i : try : entries = inverted_index . get_indexes_binary ( token , c_idx ) for s_idx , e_idx in entries : if s_idx != c_idx : continue s = S [ e_idx ] sim = self . similarity ( r_set , set ( s ), self . alpha ) if sim >= threshold : max_sim = max ( max_sim , sim ) except ValueError : continue if max_sim >= threshold : matched [ r_idx ] = max_sim return matched","title":"create_match_map"},{"location":"pages/candidate_selector/#silkmoth.candidate_selector.CandidateSelector.get_candidates","text":"Retrieve candidate set indices using token signature lookup. Parameters: Name Type Description Default signature list Signature tokens for a reference set. required inverted_index InvertedIndex Instance of the custom InvertedIndex class. required ref_size int Size of set R. required Returns: Name Type Description set set Indices of candidate sets containing at least one signature token. Source code in silkmoth/candidate_selector.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def get_candidates ( self , signature , inverted_index , ref_size ) -> set : \"\"\" Retrieve candidate set indices using token signature lookup. Args: signature (list): Signature tokens for a reference set. inverted_index (InvertedIndex): Instance of the custom InvertedIndex class. ref_size (int): Size of set R. Returns: set: Indices of candidate sets containing at least one signature token. \"\"\" candidates = set () for token in signature : try : idx_list = inverted_index . get_indexes ( token ) for set_idx , _ in idx_list : src_size = len ( inverted_index . get_set ( set_idx )) if self . verify_size ( ref_size , src_size ): candidates . add ( set_idx ) except ValueError : # token not found in inverted index; safely ignore continue return candidates","title":"get_candidates"},{"location":"pages/candidate_selector/#silkmoth.candidate_selector.CandidateSelector.nn_filter","text":"Nearest Neighbor Filter (Algorithm 2 from SilkMoth paper). Parameters: Name Type Description Default R list of list Tokenized reference set. required K set Flattened signature tokens. required candidates set Candidate indices from check filter. required inverted_index InvertedIndex To retrieve sets and indexes. required threshold float Relatedness threshold \u03b4 (between 0 and 1). required match_map dict Maps candidate set index to matched r\u1d62 indices and their max sim (from check filter). required Returns: Name Type Description set set Final filtered candidate indices that pass the NN filter. Source code in silkmoth/candidate_selector.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 def nn_filter ( self , R , K , candidates , inverted_index , threshold , match_map ) -> set : \"\"\" Nearest Neighbor Filter (Algorithm 2 from SilkMoth paper). Args: R (list of list): Tokenized reference set. K (set): Flattened signature tokens. candidates (set): Candidate indices from check filter. inverted_index (InvertedIndex): To retrieve sets and indexes. threshold (float): Relatedness threshold \u03b4 (between 0 and 1). match_map (dict): Maps candidate set index to matched r\u1d62 indices and their max sim (from check filter). Returns: set: Final filtered candidate indices that pass the NN filter. \"\"\" n = len ( R ) theta = threshold * n k_i_sets = [ set ( r_i ) . intersection ( K ) for r_i in R ] final_filtered = set () total_init = 0 for r_idx , r_i in enumerate ( R ): if not r_i : continue base_loss = ( len ( r_i ) - len ( k_i_sets [ r_idx ])) / len ( r_i ) total_init += base_loss for c_idx in candidates : S = inverted_index . get_set ( c_idx ) if self . alpha > 0 : S_tokens = set () for s in S : S_tokens . update ( s ) # Check if match_map is provided, otherwise create it if match_map is None : matched = self . create_match_map ( R , K , c_idx , inverted_index ) else : matched = match_map . get ( c_idx , {}) # Step 1: initialize total estimate total = total_init # Step 2: for matched r\u1d62, computational reuse of sim and adjust total for r_idx , sim in matched . items (): r_i = R [ r_idx ] if not r_i : continue k_i = k_i_sets [ r_idx ] base_loss = ( len ( r_i ) - len ( k_i )) / len ( r_i ) total += sim - base_loss # Step 3: for non-matched r\u1d62, compute NN and adjust total for r_idx in set ( range ( n )) - matched . keys (): r_i = R [ r_idx ] if not r_i : continue k_i = k_i_sets [ r_idx ] base_loss = ( len ( r_i ) - len ( k_i )) / len ( r_i ) r_set = set ( r_i ) # Case alpha > 0 if ( self . alpha > 0 and len ( k_i ) >= floor (( 1 - self . alpha ) * len ( r_i )) + 1 and k_i . isdisjoint ( S_tokens )): nn_sim = 0 else : nn_sim = self . _nn_search ( r_set , S , c_idx , inverted_index ) total += nn_sim - base_loss if total < theta : break if total >= theta : final_filtered . add ( c_idx ) return final_filtered","title":"nn_filter"},{"location":"pages/candidate_selector/#silkmoth.candidate_selector.CandidateSelector.verify_size","text":"Checks if sets can be related based on their sizes. Set-Containment is only defined for |R|<=|S|. For Set-Similarity we should compare only similar size sets. Parameters: Name Type Description Default ref_size int Size of set R. required src_size int Size of (possible) set S. required Returns: Name Type Description bool bool True if both sets could be related based on their size, False otherwise. Source code in silkmoth/candidate_selector.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def verify_size ( self , ref_size , src_size ) -> bool : \"\"\" Checks if sets can be related based on their sizes. Set-Containment is only defined for |R|<=|S|. For Set-Similarity we should compare only similar size sets. Args: ref_size (int): Size of set R. src_size (int): Size of (possible) set S. Returns: bool: True if both sets could be related based on their size, False otherwise. \"\"\" # case 1: Set-Containment if self . sim_metric == contain and ref_size > src_size : return False # case 2: Set-Similarity if self . sim_metric == similar : if min ( ref_size , src_size ) < self . delta * max ( ref_size , src_size ): return False return True","title":"verify_size"},{"location":"pages/inverted_index/","text":"InvertedIndex The inverted index allows to lookup all appearances of a token in a collection of tokenized sets returns inverted lists consisting of (set, element) tuples supports full sets/elements and positional indexes of them stores source sets in SilkMothEngine The inverted list is sorted first by the order of the sets and then by the order of the elements. Examples >>> from silkmoth.inverted_index import InvertedIndex >>> S1 = [{\"Apple\", \"Pear\", \"Car\"}, {\"Apple\", \"Sun\", \"Cat\"}] >>> S2 = [{\"Apple\", \"Berlin\", \"Sun\"}, {\"Apple\"}] >>> S = [S1, S2] >>> I = InvertedIndex(S) >>> I.get_indexes(\"Sun\") [(0, 1), (1, 0)] >>> I[\"Berlin\"] [([{'Sun', 'Apple', 'Berlin'}, {'Apple'}], {'Sun', 'Apple', 'Berlin'})] SilkMoth Inverted Index. Source: Deng et al., \"SILKMOTH: An Efficient Method for Finding Related Sets with Maximum Matching Constraints\", VLDB 2017. Licensed under CC BY-NC-ND 4.0. Source code in silkmoth/inverted_index.py 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 class InvertedIndex : \"\"\" The inverted index - allows to lookup all appearances of a token in a collection of tokenized sets - returns inverted lists consisting of (set, element) tuples - supports full sets/elements and positional indexes of them - stores source sets in [SilkMothEngine](silkmoth_engine.md) The inverted list - is sorted first by the order of the sets and then by the order of the elements. Examples -------- ``` >>> from silkmoth.inverted_index import InvertedIndex >>> S1 = [{\"Apple\", \"Pear\", \"Car\"}, {\"Apple\", \"Sun\", \"Cat\"}] >>> S2 = [{\"Apple\", \"Berlin\", \"Sun\"}, {\"Apple\"}] >>> S = [S1, S2] >>> I = InvertedIndex(S) >>> I.get_indexes(\"Sun\") [(0, 1), (1, 0)] >>> I[\"Berlin\"] [([{'Sun', 'Apple', 'Berlin'}, {'Apple'}], {'Sun', 'Apple', 'Berlin'})] ``` ![SilkMoth Inverted Index](../figures/InvertedIndex.png) *SilkMoth Inverted Index. Source: Deng et al., \"SILKMOTH: An Efficient Method for Finding Related Sets with Maximum Matching Constraints\", VLDB 2017. Licensed under CC BY-NC-ND 4.0.* \"\"\" def __init__ ( self , token_sets : list ): \"\"\" Initialize the inverted index. Args: token_sets (list): Collection of (tokenized) sets \"\"\" self . token_sets = token_sets self . lookup_table = dict () for set_idx , token_set in enumerate ( self . token_sets ): for element_idx , tokens in enumerate ( token_set ): for token in tokens : if not token in self . lookup_table : self . lookup_table [ token ] = [( set_idx , element_idx )] # avoid duplicates in inverted list elif self . lookup_table [ token ][ - 1 ] != ( set_idx , element_idx ): self . lookup_table [ token ] . append (( set_idx , element_idx )) def keys ( self ): \"\"\" Gives all tokens similar like dict.keys(). Returns: set (set): A set-like object providing all keys \"\"\" return self . lookup_table . keys () def __getitem__ ( self , token ) -> list : \"\"\" Access inverted list from inverted index using square brackets. Args: token (str): Input token Returns: list: A list of all (set, element) tuples which contain the input token. \"\"\" idx_list = self . get_indexes ( token ) return [( self . get_set ( s ), self . get_set ( s )[ e ]) for s , e in idx_list ] def get_indexes ( self , token ) -> list : \"\"\" Access inverted list of indexes. For some tasks retrieving the full set and element pairs might not be necessary and their indexes are sufficient. Args: token (str): Input token Returns: list: A list of all (set index, element index) tuples for (set, element) tuples which contain the input tuple \"\"\" if not token in self . lookup_table : raise ValueError ( f \"Unknown token\" ) return self . lookup_table [ token ] def get_set ( self , set_id : int ) -> list : \"\"\" Access (tokenized) set from set ID. Args: set_id: Set ID Returns: list: Tokenized set \"\"\" if set_id < 0 or set_id >= len ( self . token_sets ): raise ValueError ( f \"Invalid id\" ) return self . token_sets [ set_id ] def get_indexes_binary ( self , token , set_idx ) -> list : \"\"\" Uses binary search to get all (set_idx, element_idx) pairs for a token where set_idx matches the given set_idx. Args: token (str): The token to search in the inverted index. set_idx (int): The ID of the set we want the element indexes for. Returns: list: All (set_idx, element_idx) tuples where the token appears in the given set. \"\"\" if token not in self . lookup_table : raise ValueError ( \"Unknown token\" ) index_list = self . lookup_table [ token ] # Using bisect to find the range of entries where set_idx matches left = bisect . bisect_left ( index_list , ( set_idx , - 1 )) right = bisect . bisect_right ( index_list , ( set_idx , float ( 'inf' ))) return index_list [ left : right ] __getitem__ ( token ) Access inverted list from inverted index using square brackets. Parameters: Name Type Description Default token str Input token required Returns: Name Type Description list list A list of all (set, element) tuples which contain the input token. Source code in silkmoth/inverted_index.py 63 64 65 66 67 68 69 70 71 72 73 74 75 def __getitem__ ( self , token ) -> list : \"\"\" Access inverted list from inverted index using square brackets. Args: token (str): Input token Returns: list: A list of all (set, element) tuples which contain the input token. \"\"\" idx_list = self . get_indexes ( token ) return [( self . get_set ( s ), self . get_set ( s )[ e ]) for s , e in idx_list ] __init__ ( token_sets ) Initialize the inverted index. Parameters: Name Type Description Default token_sets list Collection of (tokenized) sets required Source code in silkmoth/inverted_index.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def __init__ ( self , token_sets : list ): \"\"\" Initialize the inverted index. Args: token_sets (list): Collection of (tokenized) sets \"\"\" self . token_sets = token_sets self . lookup_table = dict () for set_idx , token_set in enumerate ( self . token_sets ): for element_idx , tokens in enumerate ( token_set ): for token in tokens : if not token in self . lookup_table : self . lookup_table [ token ] = [( set_idx , element_idx )] # avoid duplicates in inverted list elif self . lookup_table [ token ][ - 1 ] != ( set_idx , element_idx ): self . lookup_table [ token ] . append (( set_idx , element_idx )) get_indexes ( token ) Access inverted list of indexes. For some tasks retrieving the full set and element pairs might not be necessary and their indexes are sufficient. Parameters: Name Type Description Default token str Input token required Returns: Name Type Description list list A list of all (set index, element index) tuples for (set, element) tuples which contain the input tuple Source code in silkmoth/inverted_index.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def get_indexes ( self , token ) -> list : \"\"\" Access inverted list of indexes. For some tasks retrieving the full set and element pairs might not be necessary and their indexes are sufficient. Args: token (str): Input token Returns: list: A list of all (set index, element index) tuples for (set, element) tuples which contain the input tuple \"\"\" if not token in self . lookup_table : raise ValueError ( f \"Unknown token\" ) return self . lookup_table [ token ] get_indexes_binary ( token , set_idx ) Uses binary search to get all (set_idx, element_idx) pairs for a token where set_idx matches the given set_idx. Parameters: Name Type Description Default token str The token to search in the inverted index. required set_idx int The ID of the set we want the element indexes for. required Returns: Name Type Description list list All (set_idx, element_idx) tuples where the token appears in the given set. Source code in silkmoth/inverted_index.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def get_indexes_binary ( self , token , set_idx ) -> list : \"\"\" Uses binary search to get all (set_idx, element_idx) pairs for a token where set_idx matches the given set_idx. Args: token (str): The token to search in the inverted index. set_idx (int): The ID of the set we want the element indexes for. Returns: list: All (set_idx, element_idx) tuples where the token appears in the given set. \"\"\" if token not in self . lookup_table : raise ValueError ( \"Unknown token\" ) index_list = self . lookup_table [ token ] # Using bisect to find the range of entries where set_idx matches left = bisect . bisect_left ( index_list , ( set_idx , - 1 )) right = bisect . bisect_right ( index_list , ( set_idx , float ( 'inf' ))) return index_list [ left : right ] get_set ( set_id ) Access (tokenized) set from set ID. Parameters: Name Type Description Default set_id int Set ID required Returns: Name Type Description list list Tokenized set Source code in silkmoth/inverted_index.py 94 95 96 97 98 99 100 101 102 103 104 105 106 def get_set ( self , set_id : int ) -> list : \"\"\" Access (tokenized) set from set ID. Args: set_id: Set ID Returns: list: Tokenized set \"\"\" if set_id < 0 or set_id >= len ( self . token_sets ): raise ValueError ( f \"Invalid id\" ) return self . token_sets [ set_id ] keys () Gives all tokens similar like dict.keys(). Returns: Name Type Description set set A set-like object providing all keys Source code in silkmoth/inverted_index.py 54 55 56 57 58 59 60 61 def keys ( self ): \"\"\" Gives all tokens similar like dict.keys(). Returns: set (set): A set-like object providing all keys \"\"\" return self . lookup_table . keys ()","title":"Inverted Index"},{"location":"pages/inverted_index/#silkmoth.inverted_index.InvertedIndex","text":"The inverted index allows to lookup all appearances of a token in a collection of tokenized sets returns inverted lists consisting of (set, element) tuples supports full sets/elements and positional indexes of them stores source sets in SilkMothEngine The inverted list is sorted first by the order of the sets and then by the order of the elements.","title":"InvertedIndex"},{"location":"pages/inverted_index/#silkmoth.inverted_index.InvertedIndex--examples","text":">>> from silkmoth.inverted_index import InvertedIndex >>> S1 = [{\"Apple\", \"Pear\", \"Car\"}, {\"Apple\", \"Sun\", \"Cat\"}] >>> S2 = [{\"Apple\", \"Berlin\", \"Sun\"}, {\"Apple\"}] >>> S = [S1, S2] >>> I = InvertedIndex(S) >>> I.get_indexes(\"Sun\") [(0, 1), (1, 0)] >>> I[\"Berlin\"] [([{'Sun', 'Apple', 'Berlin'}, {'Apple'}], {'Sun', 'Apple', 'Berlin'})] SilkMoth Inverted Index. Source: Deng et al., \"SILKMOTH: An Efficient Method for Finding Related Sets with Maximum Matching Constraints\", VLDB 2017. Licensed under CC BY-NC-ND 4.0. Source code in silkmoth/inverted_index.py 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 class InvertedIndex : \"\"\" The inverted index - allows to lookup all appearances of a token in a collection of tokenized sets - returns inverted lists consisting of (set, element) tuples - supports full sets/elements and positional indexes of them - stores source sets in [SilkMothEngine](silkmoth_engine.md) The inverted list - is sorted first by the order of the sets and then by the order of the elements. Examples -------- ``` >>> from silkmoth.inverted_index import InvertedIndex >>> S1 = [{\"Apple\", \"Pear\", \"Car\"}, {\"Apple\", \"Sun\", \"Cat\"}] >>> S2 = [{\"Apple\", \"Berlin\", \"Sun\"}, {\"Apple\"}] >>> S = [S1, S2] >>> I = InvertedIndex(S) >>> I.get_indexes(\"Sun\") [(0, 1), (1, 0)] >>> I[\"Berlin\"] [([{'Sun', 'Apple', 'Berlin'}, {'Apple'}], {'Sun', 'Apple', 'Berlin'})] ``` ![SilkMoth Inverted Index](../figures/InvertedIndex.png) *SilkMoth Inverted Index. Source: Deng et al., \"SILKMOTH: An Efficient Method for Finding Related Sets with Maximum Matching Constraints\", VLDB 2017. Licensed under CC BY-NC-ND 4.0.* \"\"\" def __init__ ( self , token_sets : list ): \"\"\" Initialize the inverted index. Args: token_sets (list): Collection of (tokenized) sets \"\"\" self . token_sets = token_sets self . lookup_table = dict () for set_idx , token_set in enumerate ( self . token_sets ): for element_idx , tokens in enumerate ( token_set ): for token in tokens : if not token in self . lookup_table : self . lookup_table [ token ] = [( set_idx , element_idx )] # avoid duplicates in inverted list elif self . lookup_table [ token ][ - 1 ] != ( set_idx , element_idx ): self . lookup_table [ token ] . append (( set_idx , element_idx )) def keys ( self ): \"\"\" Gives all tokens similar like dict.keys(). Returns: set (set): A set-like object providing all keys \"\"\" return self . lookup_table . keys () def __getitem__ ( self , token ) -> list : \"\"\" Access inverted list from inverted index using square brackets. Args: token (str): Input token Returns: list: A list of all (set, element) tuples which contain the input token. \"\"\" idx_list = self . get_indexes ( token ) return [( self . get_set ( s ), self . get_set ( s )[ e ]) for s , e in idx_list ] def get_indexes ( self , token ) -> list : \"\"\" Access inverted list of indexes. For some tasks retrieving the full set and element pairs might not be necessary and their indexes are sufficient. Args: token (str): Input token Returns: list: A list of all (set index, element index) tuples for (set, element) tuples which contain the input tuple \"\"\" if not token in self . lookup_table : raise ValueError ( f \"Unknown token\" ) return self . lookup_table [ token ] def get_set ( self , set_id : int ) -> list : \"\"\" Access (tokenized) set from set ID. Args: set_id: Set ID Returns: list: Tokenized set \"\"\" if set_id < 0 or set_id >= len ( self . token_sets ): raise ValueError ( f \"Invalid id\" ) return self . token_sets [ set_id ] def get_indexes_binary ( self , token , set_idx ) -> list : \"\"\" Uses binary search to get all (set_idx, element_idx) pairs for a token where set_idx matches the given set_idx. Args: token (str): The token to search in the inverted index. set_idx (int): The ID of the set we want the element indexes for. Returns: list: All (set_idx, element_idx) tuples where the token appears in the given set. \"\"\" if token not in self . lookup_table : raise ValueError ( \"Unknown token\" ) index_list = self . lookup_table [ token ] # Using bisect to find the range of entries where set_idx matches left = bisect . bisect_left ( index_list , ( set_idx , - 1 )) right = bisect . bisect_right ( index_list , ( set_idx , float ( 'inf' ))) return index_list [ left : right ]","title":"Examples"},{"location":"pages/inverted_index/#silkmoth.inverted_index.InvertedIndex.__getitem__","text":"Access inverted list from inverted index using square brackets. Parameters: Name Type Description Default token str Input token required Returns: Name Type Description list list A list of all (set, element) tuples which contain the input token. Source code in silkmoth/inverted_index.py 63 64 65 66 67 68 69 70 71 72 73 74 75 def __getitem__ ( self , token ) -> list : \"\"\" Access inverted list from inverted index using square brackets. Args: token (str): Input token Returns: list: A list of all (set, element) tuples which contain the input token. \"\"\" idx_list = self . get_indexes ( token ) return [( self . get_set ( s ), self . get_set ( s )[ e ]) for s , e in idx_list ]","title":"__getitem__"},{"location":"pages/inverted_index/#silkmoth.inverted_index.InvertedIndex.__init__","text":"Initialize the inverted index. Parameters: Name Type Description Default token_sets list Collection of (tokenized) sets required Source code in silkmoth/inverted_index.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def __init__ ( self , token_sets : list ): \"\"\" Initialize the inverted index. Args: token_sets (list): Collection of (tokenized) sets \"\"\" self . token_sets = token_sets self . lookup_table = dict () for set_idx , token_set in enumerate ( self . token_sets ): for element_idx , tokens in enumerate ( token_set ): for token in tokens : if not token in self . lookup_table : self . lookup_table [ token ] = [( set_idx , element_idx )] # avoid duplicates in inverted list elif self . lookup_table [ token ][ - 1 ] != ( set_idx , element_idx ): self . lookup_table [ token ] . append (( set_idx , element_idx ))","title":"__init__"},{"location":"pages/inverted_index/#silkmoth.inverted_index.InvertedIndex.get_indexes","text":"Access inverted list of indexes. For some tasks retrieving the full set and element pairs might not be necessary and their indexes are sufficient. Parameters: Name Type Description Default token str Input token required Returns: Name Type Description list list A list of all (set index, element index) tuples for (set, element) tuples which contain the input tuple Source code in silkmoth/inverted_index.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def get_indexes ( self , token ) -> list : \"\"\" Access inverted list of indexes. For some tasks retrieving the full set and element pairs might not be necessary and their indexes are sufficient. Args: token (str): Input token Returns: list: A list of all (set index, element index) tuples for (set, element) tuples which contain the input tuple \"\"\" if not token in self . lookup_table : raise ValueError ( f \"Unknown token\" ) return self . lookup_table [ token ]","title":"get_indexes"},{"location":"pages/inverted_index/#silkmoth.inverted_index.InvertedIndex.get_indexes_binary","text":"Uses binary search to get all (set_idx, element_idx) pairs for a token where set_idx matches the given set_idx. Parameters: Name Type Description Default token str The token to search in the inverted index. required set_idx int The ID of the set we want the element indexes for. required Returns: Name Type Description list list All (set_idx, element_idx) tuples where the token appears in the given set. Source code in silkmoth/inverted_index.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def get_indexes_binary ( self , token , set_idx ) -> list : \"\"\" Uses binary search to get all (set_idx, element_idx) pairs for a token where set_idx matches the given set_idx. Args: token (str): The token to search in the inverted index. set_idx (int): The ID of the set we want the element indexes for. Returns: list: All (set_idx, element_idx) tuples where the token appears in the given set. \"\"\" if token not in self . lookup_table : raise ValueError ( \"Unknown token\" ) index_list = self . lookup_table [ token ] # Using bisect to find the range of entries where set_idx matches left = bisect . bisect_left ( index_list , ( set_idx , - 1 )) right = bisect . bisect_right ( index_list , ( set_idx , float ( 'inf' ))) return index_list [ left : right ]","title":"get_indexes_binary"},{"location":"pages/inverted_index/#silkmoth.inverted_index.InvertedIndex.get_set","text":"Access (tokenized) set from set ID. Parameters: Name Type Description Default set_id int Set ID required Returns: Name Type Description list list Tokenized set Source code in silkmoth/inverted_index.py 94 95 96 97 98 99 100 101 102 103 104 105 106 def get_set ( self , set_id : int ) -> list : \"\"\" Access (tokenized) set from set ID. Args: set_id: Set ID Returns: list: Tokenized set \"\"\" if set_id < 0 or set_id >= len ( self . token_sets ): raise ValueError ( f \"Invalid id\" ) return self . token_sets [ set_id ]","title":"get_set"},{"location":"pages/inverted_index/#silkmoth.inverted_index.InvertedIndex.keys","text":"Gives all tokens similar like dict.keys(). Returns: Name Type Description set set A set-like object providing all keys Source code in silkmoth/inverted_index.py 54 55 56 57 58 59 60 61 def keys ( self ): \"\"\" Gives all tokens similar like dict.keys(). Returns: set (set): A set-like object providing all keys \"\"\" return self . lookup_table . keys ()","title":"keys"},{"location":"pages/signature_generator/","text":"SignatureGenerator Source code in silkmoth/signature_generator.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 class SignatureGenerator : def get_signature ( self , reference_set , inverted_index , delta , alpha = 0 , sig_type = SigType . WEIGHTED ) -> list : \"\"\" Compute a signature for a reference set given a signature type. Uses weighted signature scheme by default. Args: reference_set (list): Tokenized reference set. inverted_index (InvertedIndex): Index to evaluate token cost. delta (float): Relatedness threshold factor. alpha (float): Similarity threshold factor. sig_type (SigType): Type of signature. Returns: list: A list of str for selected tokens forming the signature. \"\"\" match sig_type : case SigType . WEIGHTED : return self . _generate_weighted_signature ( reference_set , inverted_index , delta ) case SigType . SKYLINE : return self . _generate_skyline_signature ( reference_set , inverted_index , delta , alpha ) case SigType . DICHOTOMY : return self . _generate_dichotomy_signature ( reference_set , inverted_index , delta , alpha ) case _ : raise ValueError ( f \"Unknown signature type\" ) def _generate_skyline_signature ( self , reference_set , inverted_index : InvertedIndex , delta , alpha ): weighted = set ( self . _generate_weighted_signature ( reference_set , inverted_index , delta )) unflattened = [ weighted & set ( r_i ) for r_i in reference_set ] skyline = set () for i , k in enumerate ( unflattened ): rhs = floor (( 1 - alpha ) * len ( reference_set [ i ])) + 1 if len ( k ) < rhs : skyline |= k else : # add tokens with minimum |I[t]| tokens = list ( k ) tokens . sort ( key = lambda t : len ( inverted_index . get_indexes ( t ))) skyline = skyline . union ( tokens [: rhs ]) return list ( skyline ) def _generate_dichotomy_signature ( self , reference_set , inverted_index : InvertedIndex , delta , alpha ): \"\"\" Generates a signature using the Dichotomy Scheme as described in the SILKMOTH paper. For each element r_i, it chooses between its weighted signature part (k_i) and all its tokens (r_i) based on whether k_i is a subset of an optimal sim-thresh signature (m_i). \"\"\" # 1. First, generate the optimal weighted signature, K. weighted_signature_K = set ( self . _generate_weighted_signature ( reference_set , inverted_index , delta )) final_dichotomy_sig = set () # 2. For each element r_i, decide whether to use its k_i or the full r_i. for r_i_list in reference_set : r_i = set ( r_i_list ) if not r_i : continue # 3. Determine k_i: the part of the weighted signature in this element. k_i = weighted_signature_K . intersection ( r_i ) # 4. Determine m_i: the optimal sim-thresh signature for this element. # 4a. Calculate the required size for the sim-thresh signature. m_i_size = floor (( 1 - alpha ) * len ( r_i )) + 1 # 4b. Get all tokens from the original element r_i and sort by cost. element_tokens = list ( r_i ) # Sort tokens by the length of their inverted index list (cost). # Handle cases where a token might not be in the index. def get_token_cost ( token ): try : return len ( inverted_index . get_indexes ( token )) except ValueError : return float ( 'inf' ) # Assign a high cost if not found element_tokens . sort ( key = get_token_cost ) # 4c. The optimal m_i consists of the cheapest tokens. m_i = set ( element_tokens [: m_i_size ]) # 5. The Decision: Apply the paper's condition. # Can we get away with the cheaper weighted signature part (k_i)? # Yes, if k_i is already a subset of the optimal sim-thresh signature (m_i). if k_i . issubset ( m_i ): # Decision: Use the cheaper weighted signature tokens. final_dichotomy_sig . update ( k_i ) else : # Decision: Fall back to the safe, more expensive option. final_dichotomy_sig . update ( r_i ) return list ( final_dichotomy_sig ) def _generate_weighted_signature ( self , reference_set , inverted_index , delta ): if delta <= 0.0 : return [] n = len ( reference_set ) theta = delta * n # required covered fraction , delta * |R| in paper # 1) Build token: elements map and aggregate token values token_to_elems = defaultdict ( list ) token_value = {} for i , elem in enumerate ( reference_set ): if not elem : warnings . warn ( f \"Element at index { i } is empty and will be skipped.\" ) continue unique_tokens = set ( elem ) # remove duplicate tokens inside each element weight = 1.0 / len ( unique_tokens ) for t in unique_tokens : token_to_elems [ t ] . append ( i ) token_value [ t ] = token_value . get ( t , 0.0 ) + weight # value = sum of weights (for each token) # 2) Build min-heap of (cost/value, token) heap = [] for t , val in token_value . items (): if val <= 0 : continue try : cost = len ( inverted_index . get_indexes ( t )) # look up each token in inverted index to count in how many sets it is = cost except ValueError : # Token not in index: assign infinite cost to deprioritize cost = float ( 'inf' ) heapq . heappush ( heap , ( cost / val , t )) # goal small ratio: cost/value # 3) Selection with greedy algorithm selected_sig = set () r_sizes = [ len ( set ( elem )) if elem else 0 for elem in reference_set ] total_loss = float ( n ) current_k_counts = [ 0 ] * n # while heap and total_loss >= theta: while heap and total_loss >= theta : # 1. ratio , t = heapq . heappop ( heap ) # pull best token with lowest cost/value from heap if t in selected_sig : continue if ratio == float ( 'inf' ): break # 2. selected_sig . add ( t ) # 3. for i in range ( n ): if r_sizes [ i ] == 0 : continue # Calculate |k_i|: number of tokens from reference_set[i] also in selected_sig current_k_counts [ i ] = len ( set ( reference_set [ i ]) . intersection ( selected_sig )) # 4. total_loss = sum ( ( r_sizes [ i ] - current_k_counts [ i ]) / r_sizes [ i ] for i in range ( n ) if r_sizes [ i ] > 0 ) return list ( selected_sig ) get_signature ( reference_set , inverted_index , delta , alpha = 0 , sig_type = SigType . WEIGHTED ) Compute a signature for a reference set given a signature type. Uses weighted signature scheme by default. Parameters: Name Type Description Default reference_set list Tokenized reference set. required inverted_index InvertedIndex Index to evaluate token cost. required delta float Relatedness threshold factor. required alpha float Similarity threshold factor. 0 sig_type SigType Type of signature. WEIGHTED Returns: Name Type Description list list A list of str for selected tokens forming the signature. Source code in silkmoth/signature_generator.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def get_signature ( self , reference_set , inverted_index , delta , alpha = 0 , sig_type = SigType . WEIGHTED ) -> list : \"\"\" Compute a signature for a reference set given a signature type. Uses weighted signature scheme by default. Args: reference_set (list): Tokenized reference set. inverted_index (InvertedIndex): Index to evaluate token cost. delta (float): Relatedness threshold factor. alpha (float): Similarity threshold factor. sig_type (SigType): Type of signature. Returns: list: A list of str for selected tokens forming the signature. \"\"\" match sig_type : case SigType . WEIGHTED : return self . _generate_weighted_signature ( reference_set , inverted_index , delta ) case SigType . SKYLINE : return self . _generate_skyline_signature ( reference_set , inverted_index , delta , alpha ) case SigType . DICHOTOMY : return self . _generate_dichotomy_signature ( reference_set , inverted_index , delta , alpha ) case _ : raise ValueError ( f \"Unknown signature type\" )","title":"Signature Generator"},{"location":"pages/signature_generator/#silkmoth.signature_generator.SignatureGenerator","text":"Source code in silkmoth/signature_generator.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 class SignatureGenerator : def get_signature ( self , reference_set , inverted_index , delta , alpha = 0 , sig_type = SigType . WEIGHTED ) -> list : \"\"\" Compute a signature for a reference set given a signature type. Uses weighted signature scheme by default. Args: reference_set (list): Tokenized reference set. inverted_index (InvertedIndex): Index to evaluate token cost. delta (float): Relatedness threshold factor. alpha (float): Similarity threshold factor. sig_type (SigType): Type of signature. Returns: list: A list of str for selected tokens forming the signature. \"\"\" match sig_type : case SigType . WEIGHTED : return self . _generate_weighted_signature ( reference_set , inverted_index , delta ) case SigType . SKYLINE : return self . _generate_skyline_signature ( reference_set , inverted_index , delta , alpha ) case SigType . DICHOTOMY : return self . _generate_dichotomy_signature ( reference_set , inverted_index , delta , alpha ) case _ : raise ValueError ( f \"Unknown signature type\" ) def _generate_skyline_signature ( self , reference_set , inverted_index : InvertedIndex , delta , alpha ): weighted = set ( self . _generate_weighted_signature ( reference_set , inverted_index , delta )) unflattened = [ weighted & set ( r_i ) for r_i in reference_set ] skyline = set () for i , k in enumerate ( unflattened ): rhs = floor (( 1 - alpha ) * len ( reference_set [ i ])) + 1 if len ( k ) < rhs : skyline |= k else : # add tokens with minimum |I[t]| tokens = list ( k ) tokens . sort ( key = lambda t : len ( inverted_index . get_indexes ( t ))) skyline = skyline . union ( tokens [: rhs ]) return list ( skyline ) def _generate_dichotomy_signature ( self , reference_set , inverted_index : InvertedIndex , delta , alpha ): \"\"\" Generates a signature using the Dichotomy Scheme as described in the SILKMOTH paper. For each element r_i, it chooses between its weighted signature part (k_i) and all its tokens (r_i) based on whether k_i is a subset of an optimal sim-thresh signature (m_i). \"\"\" # 1. First, generate the optimal weighted signature, K. weighted_signature_K = set ( self . _generate_weighted_signature ( reference_set , inverted_index , delta )) final_dichotomy_sig = set () # 2. For each element r_i, decide whether to use its k_i or the full r_i. for r_i_list in reference_set : r_i = set ( r_i_list ) if not r_i : continue # 3. Determine k_i: the part of the weighted signature in this element. k_i = weighted_signature_K . intersection ( r_i ) # 4. Determine m_i: the optimal sim-thresh signature for this element. # 4a. Calculate the required size for the sim-thresh signature. m_i_size = floor (( 1 - alpha ) * len ( r_i )) + 1 # 4b. Get all tokens from the original element r_i and sort by cost. element_tokens = list ( r_i ) # Sort tokens by the length of their inverted index list (cost). # Handle cases where a token might not be in the index. def get_token_cost ( token ): try : return len ( inverted_index . get_indexes ( token )) except ValueError : return float ( 'inf' ) # Assign a high cost if not found element_tokens . sort ( key = get_token_cost ) # 4c. The optimal m_i consists of the cheapest tokens. m_i = set ( element_tokens [: m_i_size ]) # 5. The Decision: Apply the paper's condition. # Can we get away with the cheaper weighted signature part (k_i)? # Yes, if k_i is already a subset of the optimal sim-thresh signature (m_i). if k_i . issubset ( m_i ): # Decision: Use the cheaper weighted signature tokens. final_dichotomy_sig . update ( k_i ) else : # Decision: Fall back to the safe, more expensive option. final_dichotomy_sig . update ( r_i ) return list ( final_dichotomy_sig ) def _generate_weighted_signature ( self , reference_set , inverted_index , delta ): if delta <= 0.0 : return [] n = len ( reference_set ) theta = delta * n # required covered fraction , delta * |R| in paper # 1) Build token: elements map and aggregate token values token_to_elems = defaultdict ( list ) token_value = {} for i , elem in enumerate ( reference_set ): if not elem : warnings . warn ( f \"Element at index { i } is empty and will be skipped.\" ) continue unique_tokens = set ( elem ) # remove duplicate tokens inside each element weight = 1.0 / len ( unique_tokens ) for t in unique_tokens : token_to_elems [ t ] . append ( i ) token_value [ t ] = token_value . get ( t , 0.0 ) + weight # value = sum of weights (for each token) # 2) Build min-heap of (cost/value, token) heap = [] for t , val in token_value . items (): if val <= 0 : continue try : cost = len ( inverted_index . get_indexes ( t )) # look up each token in inverted index to count in how many sets it is = cost except ValueError : # Token not in index: assign infinite cost to deprioritize cost = float ( 'inf' ) heapq . heappush ( heap , ( cost / val , t )) # goal small ratio: cost/value # 3) Selection with greedy algorithm selected_sig = set () r_sizes = [ len ( set ( elem )) if elem else 0 for elem in reference_set ] total_loss = float ( n ) current_k_counts = [ 0 ] * n # while heap and total_loss >= theta: while heap and total_loss >= theta : # 1. ratio , t = heapq . heappop ( heap ) # pull best token with lowest cost/value from heap if t in selected_sig : continue if ratio == float ( 'inf' ): break # 2. selected_sig . add ( t ) # 3. for i in range ( n ): if r_sizes [ i ] == 0 : continue # Calculate |k_i|: number of tokens from reference_set[i] also in selected_sig current_k_counts [ i ] = len ( set ( reference_set [ i ]) . intersection ( selected_sig )) # 4. total_loss = sum ( ( r_sizes [ i ] - current_k_counts [ i ]) / r_sizes [ i ] for i in range ( n ) if r_sizes [ i ] > 0 ) return list ( selected_sig )","title":"SignatureGenerator"},{"location":"pages/signature_generator/#silkmoth.signature_generator.SignatureGenerator.get_signature","text":"Compute a signature for a reference set given a signature type. Uses weighted signature scheme by default. Parameters: Name Type Description Default reference_set list Tokenized reference set. required inverted_index InvertedIndex Index to evaluate token cost. required delta float Relatedness threshold factor. required alpha float Similarity threshold factor. 0 sig_type SigType Type of signature. WEIGHTED Returns: Name Type Description list list A list of str for selected tokens forming the signature. Source code in silkmoth/signature_generator.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def get_signature ( self , reference_set , inverted_index , delta , alpha = 0 , sig_type = SigType . WEIGHTED ) -> list : \"\"\" Compute a signature for a reference set given a signature type. Uses weighted signature scheme by default. Args: reference_set (list): Tokenized reference set. inverted_index (InvertedIndex): Index to evaluate token cost. delta (float): Relatedness threshold factor. alpha (float): Similarity threshold factor. sig_type (SigType): Type of signature. Returns: list: A list of str for selected tokens forming the signature. \"\"\" match sig_type : case SigType . WEIGHTED : return self . _generate_weighted_signature ( reference_set , inverted_index , delta ) case SigType . SKYLINE : return self . _generate_skyline_signature ( reference_set , inverted_index , delta , alpha ) case SigType . DICHOTOMY : return self . _generate_dichotomy_signature ( reference_set , inverted_index , delta , alpha ) case _ : raise ValueError ( f \"Unknown signature type\" )","title":"get_signature"},{"location":"pages/silkmoth_engine/","text":"","title":"Engine"},{"location":"pages/tokenizer/","text":"Tokenizer Source code in silkmoth/tokenizer.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 class Tokenizer : def __init__ ( self , sim_func ): \"\"\" Initialize the Tokenizer with a similarity function. Args: sim_func (callable): The similarity function that influences tokenization behavior. \"\"\" self . sim_func = sim_func def tokenize ( self , input_set : list ) -> list : \"\"\" Tokenizes the input based on the similarity function. Args: input_set: The input set to tokenize. Returns: list: A list of str tokens extracted from the input. \"\"\" if self . sim_func == jaccard_similarity : tokens = jaccard_tokenize ( input_set ) else : raise ValueError ( \"Unsupported similarity function\" ) return tokens __init__ ( sim_func ) Initialize the Tokenizer with a similarity function. Parameters: Name Type Description Default sim_func callable The similarity function that influences tokenization behavior. required Source code in silkmoth/tokenizer.py 43 44 45 46 47 48 49 50 def __init__ ( self , sim_func ): \"\"\" Initialize the Tokenizer with a similarity function. Args: sim_func (callable): The similarity function that influences tokenization behavior. \"\"\" self . sim_func = sim_func tokenize ( input_set ) Tokenizes the input based on the similarity function. Parameters: Name Type Description Default input_set list The input set to tokenize. required Returns: Name Type Description list list A list of str tokens extracted from the input. Source code in silkmoth/tokenizer.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def tokenize ( self , input_set : list ) -> list : \"\"\" Tokenizes the input based on the similarity function. Args: input_set: The input set to tokenize. Returns: list: A list of str tokens extracted from the input. \"\"\" if self . sim_func == jaccard_similarity : tokens = jaccard_tokenize ( input_set ) else : raise ValueError ( \"Unsupported similarity function\" ) return tokens jaccard_tokenize ( input_set ) Tokenizes the input using Jaccard similarity. Parameters: Name Type Description Default input_set list The input set to tokenize. required Returns: Name Type Description list list A list of str tokens extracted from the input string. Source code in silkmoth/tokenizer.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def jaccard_tokenize ( input_set : list ) -> list : \"\"\" Tokenizes the input using Jaccard similarity. Args: input_set: The input set to tokenize. Returns: list: A list of str tokens extracted from the input string. \"\"\" tokens = [] for element in input_set : if isinstance ( element , ( str , int , float , bool )): tokens . append ( set ( str ( element ) . split ())) elif isinstance ( element , ( list , tuple )): sub_tokens = set () for sub_element in element : if isinstance ( sub_element , ( str , int , float , bool )): sub_tokens . update ( str ( sub_element ) . split ()) elif isinstance ( sub_element , ( list , tuple )): for sub_sub_element in sub_element : if isinstance ( sub_sub_element , ( str , int , float , bool )): sub_tokens . update ( str ( sub_sub_element ) . split ()) else : raise ValueError ( f \"Unsupported nested type: { type ( sub_element ) } \" ) else : raise ValueError ( f \"Unsupported nested type: { type ( sub_element ) } \" ) tokens . append ( sub_tokens ) else : raise ValueError ( f \"Unsupported element type: { type ( element ) } \" ) return tokens","title":"Tokenizer"},{"location":"pages/tokenizer/#silkmoth.tokenizer.Tokenizer","text":"Source code in silkmoth/tokenizer.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 class Tokenizer : def __init__ ( self , sim_func ): \"\"\" Initialize the Tokenizer with a similarity function. Args: sim_func (callable): The similarity function that influences tokenization behavior. \"\"\" self . sim_func = sim_func def tokenize ( self , input_set : list ) -> list : \"\"\" Tokenizes the input based on the similarity function. Args: input_set: The input set to tokenize. Returns: list: A list of str tokens extracted from the input. \"\"\" if self . sim_func == jaccard_similarity : tokens = jaccard_tokenize ( input_set ) else : raise ValueError ( \"Unsupported similarity function\" ) return tokens","title":"Tokenizer"},{"location":"pages/tokenizer/#silkmoth.tokenizer.Tokenizer.__init__","text":"Initialize the Tokenizer with a similarity function. Parameters: Name Type Description Default sim_func callable The similarity function that influences tokenization behavior. required Source code in silkmoth/tokenizer.py 43 44 45 46 47 48 49 50 def __init__ ( self , sim_func ): \"\"\" Initialize the Tokenizer with a similarity function. Args: sim_func (callable): The similarity function that influences tokenization behavior. \"\"\" self . sim_func = sim_func","title":"__init__"},{"location":"pages/tokenizer/#silkmoth.tokenizer.Tokenizer.tokenize","text":"Tokenizes the input based on the similarity function. Parameters: Name Type Description Default input_set list The input set to tokenize. required Returns: Name Type Description list list A list of str tokens extracted from the input. Source code in silkmoth/tokenizer.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def tokenize ( self , input_set : list ) -> list : \"\"\" Tokenizes the input based on the similarity function. Args: input_set: The input set to tokenize. Returns: list: A list of str tokens extracted from the input. \"\"\" if self . sim_func == jaccard_similarity : tokens = jaccard_tokenize ( input_set ) else : raise ValueError ( \"Unsupported similarity function\" ) return tokens","title":"tokenize"},{"location":"pages/tokenizer/#silkmoth.tokenizer.jaccard_tokenize","text":"Tokenizes the input using Jaccard similarity. Parameters: Name Type Description Default input_set list The input set to tokenize. required Returns: Name Type Description list list A list of str tokens extracted from the input string. Source code in silkmoth/tokenizer.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def jaccard_tokenize ( input_set : list ) -> list : \"\"\" Tokenizes the input using Jaccard similarity. Args: input_set: The input set to tokenize. Returns: list: A list of str tokens extracted from the input string. \"\"\" tokens = [] for element in input_set : if isinstance ( element , ( str , int , float , bool )): tokens . append ( set ( str ( element ) . split ())) elif isinstance ( element , ( list , tuple )): sub_tokens = set () for sub_element in element : if isinstance ( sub_element , ( str , int , float , bool )): sub_tokens . update ( str ( sub_element ) . split ()) elif isinstance ( sub_element , ( list , tuple )): for sub_sub_element in sub_element : if isinstance ( sub_sub_element , ( str , int , float , bool )): sub_tokens . update ( str ( sub_sub_element ) . split ()) else : raise ValueError ( f \"Unsupported nested type: { type ( sub_element ) } \" ) else : raise ValueError ( f \"Unsupported nested type: { type ( sub_element ) } \" ) tokens . append ( sub_tokens ) else : raise ValueError ( f \"Unsupported element type: { type ( element ) } \" ) return tokens","title":"jaccard_tokenize"},{"location":"pages/utils/","text":"contain ( reference_set_size , source_set_size , mm_score ) Computes Set-Containment metric which checks whether one set S is approximately a superset of another set R. Set pairs (R, S) with \\(|R| > |S|\\) should be filtered in advance. Set-Containment is defined as \\(contain(R, S) = mm\\_score / |R|\\) . Examples >>> from silkmoth.utils contain >>> contain(2, 3, 2) 1.0 >>> contain(2, 3, 1.5) 0.75 Parameters: Name Type Description Default reference_set_size int Size of set R required source_set_size int Size of set S required mm_score float Maximum matching score of R and S required Returns: Name Type Description float float Set-Containment Source code in silkmoth/utils.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def contain ( reference_set_size : int , source_set_size : int , mm_score : float ) -> float : \"\"\" Computes Set-Containment metric which checks whether one set S is approximately a superset of another set R. Set pairs (R, S) with $|R| > |S|$ should be filtered in advance. Set-Containment is defined as $contain(R, S) = mm\\_score / |R|$. Examples -------- ``` >>> from silkmoth.utils contain >>> contain(2, 3, 2) 1.0 >>> contain(2, 3, 1.5) 0.75 ``` Args: reference_set_size: Size of set R source_set_size: Size of set S mm_score: Maximum matching score of R and S Returns: float: Set-Containment \"\"\" if reference_set_size > source_set_size : raise ValueError ( f \"Reference set too large\" ) return mm_score / reference_set_size jaccard_similarity ( x , y , sim_thresh = 0 ) Gives the Jaccard similarity of two set-like objects. Jaccard similarity is defined as \\(Jac(x, y) = |x \\cap y|/|x \\cup y|\\) . For some applications we may want to omit pairs with low similarity. Therefore a similarity threshold \u03b1 is provided. If the similarity score does not exceed this threshold, this function returns zero. Examples >>> from silkmoth.utils import jaccard_similarity >>> x = {\"a\", \"b\", \"c\"} >>> y = {\"a\", \"b\", \"c\"} >>> jaccard_similarity(x, y) 1.0 >>> y.add(\"d\") >>> jaccard_similarity(x, y) 0.75 >>> jaccard_similarity(x, y, 0.8) 0.0 Parameters: Name Type Description Default x set Input element x required y set Input element y required sim_thresh float Similarity threshold alpha 0 Returns: Name Type Description float float Jaccard similarity score Source code in silkmoth/utils.py 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def jaccard_similarity ( x : set , y : set , sim_thresh = 0 ) -> float : \"\"\" Gives the Jaccard similarity of two set-like objects. Jaccard similarity is defined as $Jac(x, y) = |x \\cap y|/|x \\cup y|$. For some applications we may want to omit pairs with low similarity. Therefore a similarity threshold \u03b1 is provided. If the similarity score does not exceed this threshold, this function returns zero. Examples -------- ``` >>> from silkmoth.utils import jaccard_similarity >>> x = {\"a\", \"b\", \"c\"} >>> y = {\"a\", \"b\", \"c\"} >>> jaccard_similarity(x, y) 1.0 >>> y.add(\"d\") >>> jaccard_similarity(x, y) 0.75 >>> jaccard_similarity(x, y, 0.8) 0.0 ``` Args: x (set): Input element x y (set): Input element y sim_thresh (float): Similarity threshold alpha Returns: float: Jaccard similarity score \"\"\" if len ( x ) == 0 or len ( y ) == 0 : return .0 jac = len ( x & y ) / len ( x | y ) if jac >= sim_thresh : return jac return .0 similar ( reference_set_size , source_set_size , mm_score ) Computes Set-Similarity metric which checks whether two sets R and S are approximately equivalent. Set-Similarity is defined as \\(similar(R, S) = mm\\_score / (|R| + |S| - mm\\_score)\\) . Examples >>> from silkmoth.utils import similar >>> similar(3, 3, 3) 1.0 >>> similar(3, 3, 1.5) 0.3333333333333333 Parameters: Name Type Description Default reference_set_size int Size of set R required source_set_size int Size of set S required mm_score float Maximum matching score of R and S required Returns: Name Type Description float float Set-Similarity Source code in silkmoth/utils.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def similar ( reference_set_size : int , source_set_size : int , mm_score : float ) -> float : \"\"\" Computes Set-Similarity metric which checks whether two sets R and S are approximately equivalent. Set-Similarity is defined as $similar(R, S) = mm\\_score / (|R| + |S| - mm\\_score)$. Examples -------- ``` >>> from silkmoth.utils import similar >>> similar(3, 3, 3) 1.0 >>> similar(3, 3, 1.5) 0.3333333333333333 ``` Args: reference_set_size: Size of set R source_set_size: Size of set S mm_score: Maximum matching score of R and S Returns: float: Set-Similarity \"\"\" return mm_score / ( reference_set_size + source_set_size - mm_score )","title":"Utils"},{"location":"pages/utils/#silkmoth.utils.contain","text":"Computes Set-Containment metric which checks whether one set S is approximately a superset of another set R. Set pairs (R, S) with \\(|R| > |S|\\) should be filtered in advance. Set-Containment is defined as \\(contain(R, S) = mm\\_score / |R|\\) .","title":"contain"},{"location":"pages/utils/#silkmoth.utils.contain--examples","text":">>> from silkmoth.utils contain >>> contain(2, 3, 2) 1.0 >>> contain(2, 3, 1.5) 0.75 Parameters: Name Type Description Default reference_set_size int Size of set R required source_set_size int Size of set S required mm_score float Maximum matching score of R and S required Returns: Name Type Description float float Set-Containment Source code in silkmoth/utils.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def contain ( reference_set_size : int , source_set_size : int , mm_score : float ) -> float : \"\"\" Computes Set-Containment metric which checks whether one set S is approximately a superset of another set R. Set pairs (R, S) with $|R| > |S|$ should be filtered in advance. Set-Containment is defined as $contain(R, S) = mm\\_score / |R|$. Examples -------- ``` >>> from silkmoth.utils contain >>> contain(2, 3, 2) 1.0 >>> contain(2, 3, 1.5) 0.75 ``` Args: reference_set_size: Size of set R source_set_size: Size of set S mm_score: Maximum matching score of R and S Returns: float: Set-Containment \"\"\" if reference_set_size > source_set_size : raise ValueError ( f \"Reference set too large\" ) return mm_score / reference_set_size","title":"Examples"},{"location":"pages/utils/#silkmoth.utils.jaccard_similarity","text":"Gives the Jaccard similarity of two set-like objects. Jaccard similarity is defined as \\(Jac(x, y) = |x \\cap y|/|x \\cup y|\\) . For some applications we may want to omit pairs with low similarity. Therefore a similarity threshold \u03b1 is provided. If the similarity score does not exceed this threshold, this function returns zero.","title":"jaccard_similarity"},{"location":"pages/utils/#silkmoth.utils.jaccard_similarity--examples","text":">>> from silkmoth.utils import jaccard_similarity >>> x = {\"a\", \"b\", \"c\"} >>> y = {\"a\", \"b\", \"c\"} >>> jaccard_similarity(x, y) 1.0 >>> y.add(\"d\") >>> jaccard_similarity(x, y) 0.75 >>> jaccard_similarity(x, y, 0.8) 0.0 Parameters: Name Type Description Default x set Input element x required y set Input element y required sim_thresh float Similarity threshold alpha 0 Returns: Name Type Description float float Jaccard similarity score Source code in silkmoth/utils.py 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def jaccard_similarity ( x : set , y : set , sim_thresh = 0 ) -> float : \"\"\" Gives the Jaccard similarity of two set-like objects. Jaccard similarity is defined as $Jac(x, y) = |x \\cap y|/|x \\cup y|$. For some applications we may want to omit pairs with low similarity. Therefore a similarity threshold \u03b1 is provided. If the similarity score does not exceed this threshold, this function returns zero. Examples -------- ``` >>> from silkmoth.utils import jaccard_similarity >>> x = {\"a\", \"b\", \"c\"} >>> y = {\"a\", \"b\", \"c\"} >>> jaccard_similarity(x, y) 1.0 >>> y.add(\"d\") >>> jaccard_similarity(x, y) 0.75 >>> jaccard_similarity(x, y, 0.8) 0.0 ``` Args: x (set): Input element x y (set): Input element y sim_thresh (float): Similarity threshold alpha Returns: float: Jaccard similarity score \"\"\" if len ( x ) == 0 or len ( y ) == 0 : return .0 jac = len ( x & y ) / len ( x | y ) if jac >= sim_thresh : return jac return .0","title":"Examples"},{"location":"pages/utils/#silkmoth.utils.similar","text":"Computes Set-Similarity metric which checks whether two sets R and S are approximately equivalent. Set-Similarity is defined as \\(similar(R, S) = mm\\_score / (|R| + |S| - mm\\_score)\\) .","title":"similar"},{"location":"pages/utils/#silkmoth.utils.similar--examples","text":">>> from silkmoth.utils import similar >>> similar(3, 3, 3) 1.0 >>> similar(3, 3, 1.5) 0.3333333333333333 Parameters: Name Type Description Default reference_set_size int Size of set R required source_set_size int Size of set S required mm_score float Maximum matching score of R and S required Returns: Name Type Description float float Set-Similarity Source code in silkmoth/utils.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def similar ( reference_set_size : int , source_set_size : int , mm_score : float ) -> float : \"\"\" Computes Set-Similarity metric which checks whether two sets R and S are approximately equivalent. Set-Similarity is defined as $similar(R, S) = mm\\_score / (|R| + |S| - mm\\_score)$. Examples -------- ``` >>> from silkmoth.utils import similar >>> similar(3, 3, 3) 1.0 >>> similar(3, 3, 1.5) 0.3333333333333333 ``` Args: reference_set_size: Size of set R source_set_size: Size of set S mm_score: Maximum matching score of R and S Returns: float: Set-Similarity \"\"\" return mm_score / ( reference_set_size + source_set_size - mm_score )","title":"Examples"},{"location":"pages/verifier/","text":"Verifier The verifier component executes the final verification step in the SilkMoth pipeline. During verification SilMoth performs the maximum matching between every candidate set and the reference set R. The sets whose maximum matching score surpass the relatedness threshold \u03b4 are the verified related sets to R. For maximum matching computation we treat every element of the two sets as vertices of a bipartite graph and the weights of each edge determined by the similarity function. The maximum weighted matching is computed using the existing graph library NetworkX . Optionally, a triangle inequality-based reduction can be applied to further improve performance. Examples >>> from silkmoth.inverted_index import InvertedIndex >>> from silkmoth.utils import similar, jaccard_similarity >>> from silkmoth.verifier import Verifier >>> S1 = [{\"Apple\", \"Pear\", \"Car\"}, {\"Apple\", \"Sun\", \"Cat\"}] >>> S2 = [{\"Apple\", \"Berlin\", \"Sun\"}, {\"Apple\"}] >>> S = [S1, S2] >>> I = InvertedIndex(S) >>> R = [{\"Apple\"}, {\"Berlin\", \"Sun\"}] >>> verifier = Verifier(0.1, similar, jaccard_similarity) >>> verifier.get_related_sets(R, {0, 1}, I) [(0, 0.17073170731707313), (1, 0.7142857142857142)] >>> verifier = Verifier(0.7, similar, jaccard_similarity) >>> verifier.get_related_sets(R, {0, 1}, I) [(1, 0.7142857142857142)] Source code in silkmoth/verifier.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 class Verifier : \"\"\" The verifier component executes the final verification step in the SilkMoth pipeline. During verification SilMoth performs the maximum matching between every candidate set and the reference set R. The sets whose maximum matching score surpass the relatedness threshold \u03b4 are the verified related sets to R. For maximum matching computation we treat every element of the two sets as vertices of a bipartite graph and the weights of each edge determined by the similarity function. The maximum weighted matching is computed using the existing graph library [NetworkX](https://networkx.org/). Optionally, a triangle inequality-based reduction can be applied to further improve performance. Examples -------- ``` >>> from silkmoth.inverted_index import InvertedIndex >>> from silkmoth.utils import similar, jaccard_similarity >>> from silkmoth.verifier import Verifier >>> S1 = [{\"Apple\", \"Pear\", \"Car\"}, {\"Apple\", \"Sun\", \"Cat\"}] >>> S2 = [{\"Apple\", \"Berlin\", \"Sun\"}, {\"Apple\"}] >>> S = [S1, S2] >>> I = InvertedIndex(S) >>> R = [{\"Apple\"}, {\"Berlin\", \"Sun\"}] >>> verifier = Verifier(0.1, similar, jaccard_similarity) >>> verifier.get_related_sets(R, {0, 1}, I) [(0, 0.17073170731707313), (1, 0.7142857142857142)] >>> verifier = Verifier(0.7, similar, jaccard_similarity) >>> verifier.get_related_sets(R, {0, 1}, I) [(1, 0.7142857142857142)] ``` \"\"\" def __init__ ( self , related_thresh , sim_metric , sim_func , sim_thresh = 0 , reduction = False ): \"\"\" Initialize the verifier with some parameters. Args: related_thresh (float): Relatedness threshold delta sim_metric (callable): Similarity metric similar(...)/contain(...) sim_func (callable): Similarity function phi sim_thresh (float): Similarity threshold alpha reduction (bool): Flag to activate/deactivate triangle inequality reduction \"\"\" self . related_thresh = related_thresh self . sim_metric = sim_metric self . sim_func = sim_func self . sim_thresh = sim_thresh self . reduction = reduction def get_mm_score ( self , reference_set , source_set ) -> float : \"\"\" Helper function that computes the maximum weighted bipartite matching score, where elements correspond to nodes and the edges are weighted using the similarity function. Args: reference_set (list): Tokenized reference set R source_set (list): Tokenized source set S Returns: float: Maximum matching score (sum of weights of edges in the matching) \"\"\" G = nx . Graph () for r_idx , r_elem in enumerate ( reference_set ): for s_idx , s_elem in enumerate ( source_set ): w = self . sim_func ( r_elem , s_elem , self . sim_thresh ) G . add_edge ( r_idx , s_idx + len ( reference_set ), weight = w ) matching = nx . max_weight_matching ( G ) return sum ( G [ u ][ v ][ 'weight' ] for u , v in matching ) def get_relatedness ( self , reference_set , source_set ) -> float : \"\"\" Helper function that gives the relatedness score by computing the maximum weighted bipartite matching. Args: reference_set (list): Tokenized reference set R source_set (list): Tokenized source set S Returns: float: Relatedness score of R and S \"\"\" r_size = len ( reference_set ) s_size = len ( source_set ) exact_matches = 0 if self . reduction : reference_set , source_set , exact_matches = reduce_sets ( reference_set , source_set ) mm_score = self . get_mm_score ( reference_set , source_set ) + exact_matches relatedness = self . sim_metric ( r_size , s_size , mm_score ) return relatedness def get_related_sets ( self , reference_set : list , candidates : set , inverted_index : InvertedIndex ) -> list : \"\"\" Gives all candidate sets that are related to the reference set. Args: reference_set (list): Tokeinized reference set R candidates (set): Collection of indices of candidate sets inverted_index (InvertedIndex): Inverted index instance Returns: list: Pairs of indices of all related sets from the candidates and their relatedness with the reference set. \"\"\" related_sets = [] for c in candidates : source_set = inverted_index . get_set ( c ) relatedness = self . get_relatedness ( reference_set , source_set ) if relatedness >= self . related_thresh : related_sets . append (( c , relatedness )) return related_sets __init__ ( related_thresh , sim_metric , sim_func , sim_thresh = 0 , reduction = False ) Initialize the verifier with some parameters. Parameters: Name Type Description Default related_thresh float Relatedness threshold delta required sim_metric callable Similarity metric similar(...)/contain(...) required sim_func callable Similarity function phi required sim_thresh float Similarity threshold alpha 0 reduction bool Flag to activate/deactivate triangle inequality reduction False Source code in silkmoth/verifier.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __init__ ( self , related_thresh , sim_metric , sim_func , sim_thresh = 0 , reduction = False ): \"\"\" Initialize the verifier with some parameters. Args: related_thresh (float): Relatedness threshold delta sim_metric (callable): Similarity metric similar(...)/contain(...) sim_func (callable): Similarity function phi sim_thresh (float): Similarity threshold alpha reduction (bool): Flag to activate/deactivate triangle inequality reduction \"\"\" self . related_thresh = related_thresh self . sim_metric = sim_metric self . sim_func = sim_func self . sim_thresh = sim_thresh self . reduction = reduction get_mm_score ( reference_set , source_set ) Helper function that computes the maximum weighted bipartite matching score, where elements correspond to nodes and the edges are weighted using the similarity function. Parameters: Name Type Description Default reference_set list Tokenized reference set R required source_set list Tokenized source set S required Returns: Name Type Description float float Maximum matching score (sum of weights of edges in the float matching) Source code in silkmoth/verifier.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def get_mm_score ( self , reference_set , source_set ) -> float : \"\"\" Helper function that computes the maximum weighted bipartite matching score, where elements correspond to nodes and the edges are weighted using the similarity function. Args: reference_set (list): Tokenized reference set R source_set (list): Tokenized source set S Returns: float: Maximum matching score (sum of weights of edges in the matching) \"\"\" G = nx . Graph () for r_idx , r_elem in enumerate ( reference_set ): for s_idx , s_elem in enumerate ( source_set ): w = self . sim_func ( r_elem , s_elem , self . sim_thresh ) G . add_edge ( r_idx , s_idx + len ( reference_set ), weight = w ) matching = nx . max_weight_matching ( G ) return sum ( G [ u ][ v ][ 'weight' ] for u , v in matching ) get_related_sets ( reference_set , candidates , inverted_index ) Gives all candidate sets that are related to the reference set. Parameters: Name Type Description Default reference_set list Tokeinized reference set R required candidates set Collection of indices of candidate sets required inverted_index InvertedIndex Inverted index instance required Returns: Name Type Description list list Pairs of indices of all related sets from the candidates and list their relatedness with the reference set. Source code in silkmoth/verifier.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def get_related_sets ( self , reference_set : list , candidates : set , inverted_index : InvertedIndex ) -> list : \"\"\" Gives all candidate sets that are related to the reference set. Args: reference_set (list): Tokeinized reference set R candidates (set): Collection of indices of candidate sets inverted_index (InvertedIndex): Inverted index instance Returns: list: Pairs of indices of all related sets from the candidates and their relatedness with the reference set. \"\"\" related_sets = [] for c in candidates : source_set = inverted_index . get_set ( c ) relatedness = self . get_relatedness ( reference_set , source_set ) if relatedness >= self . related_thresh : related_sets . append (( c , relatedness )) return related_sets get_relatedness ( reference_set , source_set ) Helper function that gives the relatedness score by computing the maximum weighted bipartite matching. Parameters: Name Type Description Default reference_set list Tokenized reference set R required source_set list Tokenized source set S required Returns: Name Type Description float float Relatedness score of R and S Source code in silkmoth/verifier.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def get_relatedness ( self , reference_set , source_set ) -> float : \"\"\" Helper function that gives the relatedness score by computing the maximum weighted bipartite matching. Args: reference_set (list): Tokenized reference set R source_set (list): Tokenized source set S Returns: float: Relatedness score of R and S \"\"\" r_size = len ( reference_set ) s_size = len ( source_set ) exact_matches = 0 if self . reduction : reference_set , source_set , exact_matches = reduce_sets ( reference_set , source_set ) mm_score = self . get_mm_score ( reference_set , source_set ) + exact_matches relatedness = self . sim_metric ( r_size , s_size , mm_score ) return relatedness reduce_sets ( reference_set , source_set ) Applies the triangle inequality reduction by removing every element from both sets that has an identical match in the other set. Parameters: Name Type Description Default reference_set list Tokenized reference set R required source_set list Tokenized source set S required Returns: Type Description ( list , list , int ) Reduced reference set, reduced source set and number tuple of identical elements. Source code in silkmoth/verifier.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def reduce_sets ( reference_set : list , source_set : list ) -> tuple : \"\"\" Applies the triangle inequality reduction by removing every element from both sets that has an identical match in the other set. Args: reference_set: Tokenized reference set R source_set: Tokenized source set S Returns: (list, list, int): Reduced reference set, reduced source set and number of identical elements. \"\"\" r_reduced = reference_set [:] s_reduced = source_set [:] count = 0 for elem in reference_set : if elem in s_reduced : s_reduced . remove ( elem ) r_reduced . remove ( elem ) count += 1 return ( r_reduced , s_reduced , count )","title":"Verifier"},{"location":"pages/verifier/#silkmoth.verifier.Verifier","text":"The verifier component executes the final verification step in the SilkMoth pipeline. During verification SilMoth performs the maximum matching between every candidate set and the reference set R. The sets whose maximum matching score surpass the relatedness threshold \u03b4 are the verified related sets to R. For maximum matching computation we treat every element of the two sets as vertices of a bipartite graph and the weights of each edge determined by the similarity function. The maximum weighted matching is computed using the existing graph library NetworkX . Optionally, a triangle inequality-based reduction can be applied to further improve performance.","title":"Verifier"},{"location":"pages/verifier/#silkmoth.verifier.Verifier--examples","text":">>> from silkmoth.inverted_index import InvertedIndex >>> from silkmoth.utils import similar, jaccard_similarity >>> from silkmoth.verifier import Verifier >>> S1 = [{\"Apple\", \"Pear\", \"Car\"}, {\"Apple\", \"Sun\", \"Cat\"}] >>> S2 = [{\"Apple\", \"Berlin\", \"Sun\"}, {\"Apple\"}] >>> S = [S1, S2] >>> I = InvertedIndex(S) >>> R = [{\"Apple\"}, {\"Berlin\", \"Sun\"}] >>> verifier = Verifier(0.1, similar, jaccard_similarity) >>> verifier.get_related_sets(R, {0, 1}, I) [(0, 0.17073170731707313), (1, 0.7142857142857142)] >>> verifier = Verifier(0.7, similar, jaccard_similarity) >>> verifier.get_related_sets(R, {0, 1}, I) [(1, 0.7142857142857142)] Source code in silkmoth/verifier.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 class Verifier : \"\"\" The verifier component executes the final verification step in the SilkMoth pipeline. During verification SilMoth performs the maximum matching between every candidate set and the reference set R. The sets whose maximum matching score surpass the relatedness threshold \u03b4 are the verified related sets to R. For maximum matching computation we treat every element of the two sets as vertices of a bipartite graph and the weights of each edge determined by the similarity function. The maximum weighted matching is computed using the existing graph library [NetworkX](https://networkx.org/). Optionally, a triangle inequality-based reduction can be applied to further improve performance. Examples -------- ``` >>> from silkmoth.inverted_index import InvertedIndex >>> from silkmoth.utils import similar, jaccard_similarity >>> from silkmoth.verifier import Verifier >>> S1 = [{\"Apple\", \"Pear\", \"Car\"}, {\"Apple\", \"Sun\", \"Cat\"}] >>> S2 = [{\"Apple\", \"Berlin\", \"Sun\"}, {\"Apple\"}] >>> S = [S1, S2] >>> I = InvertedIndex(S) >>> R = [{\"Apple\"}, {\"Berlin\", \"Sun\"}] >>> verifier = Verifier(0.1, similar, jaccard_similarity) >>> verifier.get_related_sets(R, {0, 1}, I) [(0, 0.17073170731707313), (1, 0.7142857142857142)] >>> verifier = Verifier(0.7, similar, jaccard_similarity) >>> verifier.get_related_sets(R, {0, 1}, I) [(1, 0.7142857142857142)] ``` \"\"\" def __init__ ( self , related_thresh , sim_metric , sim_func , sim_thresh = 0 , reduction = False ): \"\"\" Initialize the verifier with some parameters. Args: related_thresh (float): Relatedness threshold delta sim_metric (callable): Similarity metric similar(...)/contain(...) sim_func (callable): Similarity function phi sim_thresh (float): Similarity threshold alpha reduction (bool): Flag to activate/deactivate triangle inequality reduction \"\"\" self . related_thresh = related_thresh self . sim_metric = sim_metric self . sim_func = sim_func self . sim_thresh = sim_thresh self . reduction = reduction def get_mm_score ( self , reference_set , source_set ) -> float : \"\"\" Helper function that computes the maximum weighted bipartite matching score, where elements correspond to nodes and the edges are weighted using the similarity function. Args: reference_set (list): Tokenized reference set R source_set (list): Tokenized source set S Returns: float: Maximum matching score (sum of weights of edges in the matching) \"\"\" G = nx . Graph () for r_idx , r_elem in enumerate ( reference_set ): for s_idx , s_elem in enumerate ( source_set ): w = self . sim_func ( r_elem , s_elem , self . sim_thresh ) G . add_edge ( r_idx , s_idx + len ( reference_set ), weight = w ) matching = nx . max_weight_matching ( G ) return sum ( G [ u ][ v ][ 'weight' ] for u , v in matching ) def get_relatedness ( self , reference_set , source_set ) -> float : \"\"\" Helper function that gives the relatedness score by computing the maximum weighted bipartite matching. Args: reference_set (list): Tokenized reference set R source_set (list): Tokenized source set S Returns: float: Relatedness score of R and S \"\"\" r_size = len ( reference_set ) s_size = len ( source_set ) exact_matches = 0 if self . reduction : reference_set , source_set , exact_matches = reduce_sets ( reference_set , source_set ) mm_score = self . get_mm_score ( reference_set , source_set ) + exact_matches relatedness = self . sim_metric ( r_size , s_size , mm_score ) return relatedness def get_related_sets ( self , reference_set : list , candidates : set , inverted_index : InvertedIndex ) -> list : \"\"\" Gives all candidate sets that are related to the reference set. Args: reference_set (list): Tokeinized reference set R candidates (set): Collection of indices of candidate sets inverted_index (InvertedIndex): Inverted index instance Returns: list: Pairs of indices of all related sets from the candidates and their relatedness with the reference set. \"\"\" related_sets = [] for c in candidates : source_set = inverted_index . get_set ( c ) relatedness = self . get_relatedness ( reference_set , source_set ) if relatedness >= self . related_thresh : related_sets . append (( c , relatedness )) return related_sets","title":"Examples"},{"location":"pages/verifier/#silkmoth.verifier.Verifier.__init__","text":"Initialize the verifier with some parameters. Parameters: Name Type Description Default related_thresh float Relatedness threshold delta required sim_metric callable Similarity metric similar(...)/contain(...) required sim_func callable Similarity function phi required sim_thresh float Similarity threshold alpha 0 reduction bool Flag to activate/deactivate triangle inequality reduction False Source code in silkmoth/verifier.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __init__ ( self , related_thresh , sim_metric , sim_func , sim_thresh = 0 , reduction = False ): \"\"\" Initialize the verifier with some parameters. Args: related_thresh (float): Relatedness threshold delta sim_metric (callable): Similarity metric similar(...)/contain(...) sim_func (callable): Similarity function phi sim_thresh (float): Similarity threshold alpha reduction (bool): Flag to activate/deactivate triangle inequality reduction \"\"\" self . related_thresh = related_thresh self . sim_metric = sim_metric self . sim_func = sim_func self . sim_thresh = sim_thresh self . reduction = reduction","title":"__init__"},{"location":"pages/verifier/#silkmoth.verifier.Verifier.get_mm_score","text":"Helper function that computes the maximum weighted bipartite matching score, where elements correspond to nodes and the edges are weighted using the similarity function. Parameters: Name Type Description Default reference_set list Tokenized reference set R required source_set list Tokenized source set S required Returns: Name Type Description float float Maximum matching score (sum of weights of edges in the float matching) Source code in silkmoth/verifier.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def get_mm_score ( self , reference_set , source_set ) -> float : \"\"\" Helper function that computes the maximum weighted bipartite matching score, where elements correspond to nodes and the edges are weighted using the similarity function. Args: reference_set (list): Tokenized reference set R source_set (list): Tokenized source set S Returns: float: Maximum matching score (sum of weights of edges in the matching) \"\"\" G = nx . Graph () for r_idx , r_elem in enumerate ( reference_set ): for s_idx , s_elem in enumerate ( source_set ): w = self . sim_func ( r_elem , s_elem , self . sim_thresh ) G . add_edge ( r_idx , s_idx + len ( reference_set ), weight = w ) matching = nx . max_weight_matching ( G ) return sum ( G [ u ][ v ][ 'weight' ] for u , v in matching )","title":"get_mm_score"},{"location":"pages/verifier/#silkmoth.verifier.Verifier.get_related_sets","text":"Gives all candidate sets that are related to the reference set. Parameters: Name Type Description Default reference_set list Tokeinized reference set R required candidates set Collection of indices of candidate sets required inverted_index InvertedIndex Inverted index instance required Returns: Name Type Description list list Pairs of indices of all related sets from the candidates and list their relatedness with the reference set. Source code in silkmoth/verifier.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def get_related_sets ( self , reference_set : list , candidates : set , inverted_index : InvertedIndex ) -> list : \"\"\" Gives all candidate sets that are related to the reference set. Args: reference_set (list): Tokeinized reference set R candidates (set): Collection of indices of candidate sets inverted_index (InvertedIndex): Inverted index instance Returns: list: Pairs of indices of all related sets from the candidates and their relatedness with the reference set. \"\"\" related_sets = [] for c in candidates : source_set = inverted_index . get_set ( c ) relatedness = self . get_relatedness ( reference_set , source_set ) if relatedness >= self . related_thresh : related_sets . append (( c , relatedness )) return related_sets","title":"get_related_sets"},{"location":"pages/verifier/#silkmoth.verifier.Verifier.get_relatedness","text":"Helper function that gives the relatedness score by computing the maximum weighted bipartite matching. Parameters: Name Type Description Default reference_set list Tokenized reference set R required source_set list Tokenized source set S required Returns: Name Type Description float float Relatedness score of R and S Source code in silkmoth/verifier.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def get_relatedness ( self , reference_set , source_set ) -> float : \"\"\" Helper function that gives the relatedness score by computing the maximum weighted bipartite matching. Args: reference_set (list): Tokenized reference set R source_set (list): Tokenized source set S Returns: float: Relatedness score of R and S \"\"\" r_size = len ( reference_set ) s_size = len ( source_set ) exact_matches = 0 if self . reduction : reference_set , source_set , exact_matches = reduce_sets ( reference_set , source_set ) mm_score = self . get_mm_score ( reference_set , source_set ) + exact_matches relatedness = self . sim_metric ( r_size , s_size , mm_score ) return relatedness","title":"get_relatedness"},{"location":"pages/verifier/#silkmoth.verifier.reduce_sets","text":"Applies the triangle inequality reduction by removing every element from both sets that has an identical match in the other set. Parameters: Name Type Description Default reference_set list Tokenized reference set R required source_set list Tokenized source set S required Returns: Type Description ( list , list , int ) Reduced reference set, reduced source set and number tuple of identical elements. Source code in silkmoth/verifier.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def reduce_sets ( reference_set : list , source_set : list ) -> tuple : \"\"\" Applies the triangle inequality reduction by removing every element from both sets that has an identical match in the other set. Args: reference_set: Tokenized reference set R source_set: Tokenized source set S Returns: (list, list, int): Reduced reference set, reduced source set and number of identical elements. \"\"\" r_reduced = reference_set [:] s_reduced = source_set [:] count = 0 for elem in reference_set : if elem in s_reduced : s_reduced . remove ( elem ) r_reduced . remove ( elem ) count += 1 return ( r_reduced , s_reduced , count )","title":"reduce_sets"}]}